"""
å†å²æ•°æ®å›å¡«è„šæœ¬
ä»CoinGecko APIè·å–æ›´å¤šBTCå†å²æ•°æ®
"""

import os
import time
import requests
import psycopg2
from datetime import datetime, timedelta
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HistoricalDataBackfill:
    
    def __init__(self):
        self.db_url = os.environ.get('DATABASE_URL')
        self.coingecko_base = "https://api.coingecko.com/api/v3"
        
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return psycopg2.connect(self.db_url)
    
    def fetch_historical_prices(self, days: int = 365):
        """ä»CoinGeckoè·å–å†å²ä»·æ ¼æ•°æ®"""
        url = f"{self.coingecko_base}/coins/bitcoin/market_chart"
        params = {
            'vs_currency': 'usd',
            'days': days,
            'interval': 'daily'  # æ¯æ—¥æ•°æ®ç‚¹
        }
        
        try:
            response = requests.get(url, params=params)
            response.raise_for_status()
            data = response.json()
            
            logger.info(f"æˆåŠŸè·å–{days}å¤©å†å²æ•°æ®")
            return data
        except Exception as e:
            logger.error(f"è·å–å†å²æ•°æ®å¤±è´¥: {e}")
            return None
    
    def fetch_blockchain_stats(self, timestamp: int):
        """è·å–ç‰¹å®šæ—¶é—´çš„åŒºå—é“¾ç»Ÿè®¡æ•°æ®"""
        # Blockchain.infoä¸æä¾›å†å²æ•°æ®APIï¼Œä½¿ç”¨ä¼°ç®—å€¼
        # åŸºäºå½“å‰913.66EH/så’Œéš¾åº¦è°ƒæ•´å‘¨æœŸä¼°ç®—
        
        current_hashrate = 913.66
        current_difficulty = 129435235580344.0
        
        # ç®€åŒ–ä¼°ç®—ï¼šæ ¹æ®æ—¶é—´è·ç¦»è°ƒæ•´ç®—åŠ›
        days_from_now = (time.time() - timestamp) / 86400
        hashrate_growth_rate = 0.0003  # æ¯æ—¥0.03%å¢é•¿ç‡
        
        estimated_hashrate = current_hashrate * (1 - hashrate_growth_rate) ** days_from_now
        estimated_difficulty = current_difficulty * (estimated_hashrate / current_hashrate)
        
        return {
            'hashrate': max(100, estimated_hashrate),  # æœ€å°100EH/s
            'difficulty': max(1e12, estimated_difficulty)  # æœ€å°éš¾åº¦
        }
    
    def backfill_data(self, days: int = 365):
        """å›å¡«å†å²æ•°æ®"""
        logger.info(f"å¼€å§‹å›å¡«{days}å¤©å†å²æ•°æ®...")
        
        # è·å–å†å²ä»·æ ¼æ•°æ®
        data = self.fetch_historical_prices(days)
        if not data:
            return False
        
        prices = data['prices']
        volumes = data['total_volumes']
        market_caps = data['market_caps']
        
        logger.info(f"è·å–åˆ°{len(prices)}ä¸ªæ•°æ®ç‚¹")
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # æ£€æŸ¥å·²å­˜åœ¨çš„æ•°æ®
        cursor.execute("""
            SELECT MIN(recorded_at), MAX(recorded_at), COUNT(*) 
            FROM market_analytics
        """)
        existing_min, existing_max, existing_count = cursor.fetchone()
        
        logger.info(f"æ•°æ®åº“ç°æœ‰æ•°æ®ï¼š{existing_count}æ¡ï¼Œæ—¶é—´èŒƒå›´ï¼š{existing_min} åˆ° {existing_max}")
        
        inserted_count = 0
        skipped_count = 0
        
        for i, (price_data, volume_data, cap_data) in enumerate(zip(prices, volumes, market_caps)):
            timestamp_ms, price = price_data
            _, volume = volume_data  
            _, market_cap = cap_data
            
            # è½¬æ¢æ—¶é—´æˆ³
            dt = datetime.fromtimestamp(timestamp_ms / 1000)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥æ—¥æœŸçš„æ•°æ®
            cursor.execute("""
                SELECT COUNT(*) FROM market_analytics 
                WHERE DATE(recorded_at) = %s
            """, (dt.date(),))
            
            if cursor.fetchone()[0] > 0:
                skipped_count += 1
                continue
            
            # è·å–åŒºå—é“¾ç»Ÿè®¡æ•°æ®
            blockchain_stats = self.fetch_blockchain_stats(timestamp_ms / 1000)
            
            # æ’å…¥æ•°æ®
            cursor.execute("""
                INSERT INTO market_analytics (
                    recorded_at, btc_price, btc_market_cap, btc_volume_24h,
                    network_hashrate, network_difficulty, 
                    fear_greed_index, source_apis
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
            """, (
                dt,
                float(price),
                float(market_cap), 
                float(volume),
                blockchain_stats['hashrate'],
                blockchain_stats['difficulty'],
                50,  # é»˜è®¤ææƒ§è´ªå©ªæŒ‡æ•°
                'coingecko_historical'
            ))
            
            inserted_count += 1
            
            # æ¯100æ¡æäº¤ä¸€æ¬¡
            if inserted_count % 100 == 0:
                conn.commit()
                logger.info(f"å·²æ’å…¥{inserted_count}æ¡å†å²æ•°æ®...")
                time.sleep(0.1)  # é¿å…APIé™åˆ¶
        
        # æœ€ç»ˆæäº¤
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info(f"å†å²æ•°æ®å›å¡«å®Œæˆï¼æ’å…¥{inserted_count}æ¡æ–°æ•°æ®ï¼Œè·³è¿‡{skipped_count}æ¡å·²å­˜åœ¨æ•°æ®")
        return True
    
    def extend_historical_data(self, additional_days: int = 365):
        """æ‰©å±•å†å²æ•°æ®åˆ°æ›´æ—©çš„æ—¶é—´"""
        logger.info(f"æ‰©å±•å†å²æ•°æ®ï¼Œå¢åŠ {additional_days}å¤©...")
        
        conn = self.get_connection()
        cursor = conn.cursor()
        
        # è·å–å½“å‰æœ€æ—©çš„æ•°æ®æ—¥æœŸ
        cursor.execute("""
            SELECT MIN(DATE(recorded_at)) FROM market_analytics
        """)
        earliest_date = cursor.fetchone()[0]
        
        if earliest_date:
            logger.info(f"å½“å‰æœ€æ—©æ•°æ®æ—¥æœŸï¼š{earliest_date}")
            
            # è®¡ç®—éœ€è¦è·å–çš„æ€»å¤©æ•°
            days_from_earliest = (datetime.now().date() - earliest_date).days + additional_days
            
            logger.info(f"å°†è·å–{days_from_earliest}å¤©çš„å®Œæ•´å†å²æ•°æ®")
            
            # æ¸…é™¤ç°æœ‰æ•°æ®ä»¥é¿å…é‡å¤ï¼ˆå¯é€‰ï¼‰
            # cursor.execute("DELETE FROM market_analytics WHERE source_apis = 'coingecko_historical'")
            
            # è·å–æ‰©å±•çš„å†å²æ•°æ®
            return self.backfill_data(days_from_earliest)
        else:
            logger.info("æ•°æ®åº“ä¸ºç©ºï¼Œè·å–365å¤©å†å²æ•°æ®")
            return self.backfill_data(365)

def main():
    backfill = HistoricalDataBackfill()
    
    print("=== BTCå†å²æ•°æ®æ‰©å±•å·¥å…· ===")
    print("1. å›å¡«æœ€è¿‘365å¤©æ•°æ®")
    print("2. æ‰©å±•é¢å¤–365å¤©å†å²æ•°æ®") 
    print("3. æ‰©å±•é¢å¤–730å¤©å†å²æ•°æ®ï¼ˆ2å¹´ï¼‰")
    
    choice = input("è¯·é€‰æ‹©æ“ä½œ (1-3): ").strip()
    
    if choice == "1":
        success = backfill.backfill_data(365)
    elif choice == "2":
        success = backfill.extend_historical_data(365)
    elif choice == "3":
        success = backfill.extend_historical_data(730)
    else:
        print("æ— æ•ˆé€‰æ‹©")
        return
    
    if success:
        print("âœ… å†å²æ•°æ®æ‰©å±•æˆåŠŸï¼")
        
        # æ˜¾ç¤ºæœ€æ–°ç»Ÿè®¡
        conn = backfill.get_connection()
        cursor = conn.cursor()
        cursor.execute("""
            SELECT 
                COUNT(*) as total_records,
                MIN(DATE(recorded_at)) as earliest_date,
                MAX(DATE(recorded_at)) as latest_date,
                COUNT(DISTINCT DATE(recorded_at)) as unique_days
            FROM market_analytics
        """)
        total, earliest, latest, unique_days = cursor.fetchone()
        
        print(f"ğŸ“Š æ•°æ®åº“ç»Ÿè®¡ï¼š")
        print(f"   æ€»è®°å½•æ•°: {total:,}")
        print(f"   æ—¶é—´è·¨åº¦: {earliest} åˆ° {latest}")
        print(f"   è¦†ç›–å¤©æ•°: {unique_days}")
        print(f"   å¹³å‡æ¯å¤©: {total/unique_days:.1f}æ¡è®°å½•")
        
        cursor.close()
        conn.close()
    else:
        print("âŒ å†å²æ•°æ®æ‰©å±•å¤±è´¥")

if __name__ == "__main__":
    main()