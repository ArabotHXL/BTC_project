"""
ä¼˜åŒ–çš„æ‰¹é‡æ•°æ®å¤„ç†å™¨
æå‡æ•°æ®æ”¶é›†é¢‘ç‡å’Œè´¨é‡
"""

import os
import time
import json
import logging
import requests
import psycopg2
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import schedule
from concurrent.futures import ThreadPoolExecutor, as_completed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedBatchProcessor:
    
    def __init__(self):
        self.db_url = os.environ.get('DATABASE_URL')
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'BTC-Mining-Calculator/2.0'
        })
        
        # æ•°æ®æºé…ç½®
        self.data_sources = {
            'coingecko': {
                'url': 'https://api.coingecko.com/api/v3/simple/price',
                'params': {
                    'ids': 'bitcoin',
                    'vs_currencies': 'usd',
                    'include_24hr_vol': 'true',
                    'include_24hr_change': 'true',
                    'include_market_cap': 'true'
                },
                'priority': 1
            },
            'coinbase': {
                'url': 'https://api.coinbase.com/v2/exchange-rates',
                'params': {'currency': 'BTC'},
                'priority': 2
            },
            'blockchain_info': {
                'url': 'https://blockchain.info/ticker',
                'priority': 3
            }
        }
        
        self.collection_stats = {
            'total_collections': 0,
            'successful_collections': 0,
            'failed_collections': 0,
            'last_collection': None
        }
    
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        return psycopg2.connect(self.db_url)
    
    def collect_enhanced_price_data(self) -> Optional[Dict]:
        """å¢å¼ºçš„ä»·æ ¼æ•°æ®æ”¶é›†ï¼Œå¤šæºéªŒè¯"""
        collected_data = {}
        
        # å¹¶è¡Œæ”¶é›†å¤šä¸ªæ•°æ®æº
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = {}
            
            # CoinGeckoä¸»è¦æ•°æ®æº
            try:
                params = self.data_sources['coingecko']['params']
                futures[executor.submit(self.session.get, 
                                      self.data_sources['coingecko']['url'], 
                                      params=params, timeout=10)] = 'coingecko'
            except Exception as e:
                logger.debug(f"CoinGeckoè¯·æ±‚æäº¤å¤±è´¥: {e}")
            
            # Blockchain.infoå¤‡ç”¨æº
            try:
                futures[executor.submit(self.session.get, 
                                      self.data_sources['blockchain_info']['url'], 
                                      timeout=10)] = 'blockchain_info'
            except Exception as e:
                logger.debug(f"Blockchain.infoè¯·æ±‚æäº¤å¤±è´¥: {e}")
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(futures, timeout=15):
                source = futures[future]
                try:
                    response = future.result()
                    if response.status_code == 200:
                        if source == 'coingecko':
                            data = response.json()
                            if 'bitcoin' in data:
                                btc_data = data['bitcoin']
                                collected_data['coingecko'] = {
                                    'price': btc_data.get('usd', 0),
                                    'volume_24h': btc_data.get('usd_24h_vol', 0),
                                    'price_change_24h': btc_data.get('usd_24h_change', 0),
                                    'market_cap': btc_data.get('usd_market_cap', 0)
                                }
                        
                        elif source == 'blockchain_info':
                            data = response.json()
                            if 'USD' in data:
                                usd_data = data['USD']
                                collected_data['blockchain_info'] = {
                                    'price': usd_data.get('last', 0),
                                    'high': usd_data.get('high', 0),
                                    'low': usd_data.get('low', 0)
                                }
                except Exception as e:
                    logger.debug(f"{source}æ•°æ®è§£æå¤±è´¥: {e}")
        
        # æ•°æ®éªŒè¯å’Œèšåˆ
        if collected_data:
            return self.aggregate_price_data(collected_data)
        else:
            logger.warning("æ‰€æœ‰ä»·æ ¼æ•°æ®æºå‡å¤±è´¥")
            return None
    
    def aggregate_price_data(self, collected_data: Dict) -> Dict:
        """èšåˆå’ŒéªŒè¯ä»·æ ¼æ•°æ®"""
        aggregated = {
            'sources_count': len(collected_data),
            'data_quality_score': 0,
            'consensus_price': 0,
            'volume_24h': 0,
            'price_change_24h': 0,
            'market_cap': 0
        }
        
        prices = []
        volumes = []
        
        # æå–ä»·æ ¼æ•°æ®
        if 'coingecko' in collected_data:
            cg_data = collected_data['coingecko']
            if cg_data['price'] > 0:
                prices.append(cg_data['price'])
                aggregated['volume_24h'] = cg_data.get('volume_24h', 0)
                aggregated['price_change_24h'] = cg_data.get('price_change_24h', 0)
                aggregated['market_cap'] = cg_data.get('market_cap', 0)
        
        if 'blockchain_info' in collected_data:
            bi_data = collected_data['blockchain_info']
            if bi_data['price'] > 0:
                prices.append(bi_data['price'])
        
        # è®¡ç®—å…±è¯†ä»·æ ¼ï¼ˆå¤šæºå¹³å‡ï¼‰
        if prices:
            aggregated['consensus_price'] = sum(prices) / len(prices)
            
            # ä»·æ ¼ä¸€è‡´æ€§æ£€æŸ¥
            if len(prices) > 1:
                price_variance = max(prices) - min(prices)
                price_consistency = 1 - (price_variance / aggregated['consensus_price'])
                aggregated['data_quality_score'] = int(price_consistency * 100)
            else:
                aggregated['data_quality_score'] = 85  # å•ä¸€æ•°æ®æº
        
        logger.info(f"ä»·æ ¼æ•°æ®èšåˆ: ${aggregated['consensus_price']:,.2f} (è´¨é‡åˆ†:{aggregated['data_quality_score']:.1f}%)")
        return aggregated
    
    def collect_network_stats(self) -> Optional[Dict]:
        """æ”¶é›†ç½‘ç»œç»Ÿè®¡æ•°æ®"""
        try:
            # ä»å¤šä¸ªAPIè·å–ç½‘ç»œæ•°æ®
            network_data = {}
            
            # Blockchain.info stats
            try:
                response = self.session.get('https://blockchain.info/stats?format=json', timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    network_data['blockchain_info'] = {
                        'difficulty': data.get('difficulty', 0),
                        'hashrate': data.get('hash_rate', 0),
                        'total_btc': data.get('totalbc', 0) / 100000000,  # satoshi to BTC
                        'blocks_count': data.get('n_blocks_total', 0)
                    }
            except Exception as e:
                logger.debug(f"Blockchain.info statså¤±è´¥: {e}")
            
            # Mempool.space difficulty
            try:
                response = self.session.get('https://mempool.space/api/v1/difficulty-adjustment', timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    network_data['mempool'] = {
                        'current_difficulty': data.get('difficultyChange', 0),
                        'estimated_hashrate': data.get('currentHashrate', 0) / 1e18,  # H/s to EH/s
                        'blocks_until_adjustment': data.get('blocksUntilRetarget', 0)
                    }
            except Exception as e:
                logger.debug(f"Mempool.space difficultyå¤±è´¥: {e}")
            
            if network_data:
                return self.process_network_data(network_data)
            else:
                return None
                
        except Exception as e:
            logger.error(f"ç½‘ç»œæ•°æ®æ”¶é›†å¤±è´¥: {e}")
            return None
    
    def process_network_data(self, network_data: Dict) -> Dict:
        """å¤„ç†ç½‘ç»œæ•°æ®"""
        processed = {
            'network_hashrate': 0,
            'network_difficulty': 0,
            'total_supply': 19700000,  # é»˜è®¤å½“å‰ä¾›åº”é‡
            'data_sources': list(network_data.keys())
        }
        
        # ä¼˜å…ˆä½¿ç”¨Blockchain.infoæ•°æ®
        if 'blockchain_info' in network_data:
            bi_data = network_data['blockchain_info']
            processed['network_difficulty'] = bi_data.get('difficulty', 0)
            
            # ç®—åŠ›å¤„ç†
            hashrate_raw = bi_data.get('hashrate', 0)
            if hashrate_raw > 1e15:  # å¦‚æœæ˜¯H/s
                processed['network_hashrate'] = hashrate_raw / 1e18
            elif hashrate_raw > 1e6:  # å¦‚æœæ˜¯GH/s  
                processed['network_hashrate'] = hashrate_raw / 1e9
            else:  # å‡è®¾æ˜¯EH/sæˆ–å…¶ä»–
                processed['network_hashrate'] = hashrate_raw
                
            processed['total_supply'] = bi_data.get('total_btc', 19700000)
        
        # ä½¿ç”¨Mempool.spaceä½œä¸ºéªŒè¯
        if 'mempool' in network_data:
            mp_data = network_data['mempool']
            mempool_hashrate = mp_data.get('estimated_hashrate', 0)
            
            if mempool_hashrate > 0 and processed['network_hashrate'] == 0:
                processed['network_hashrate'] = mempool_hashrate
            elif mempool_hashrate > 0:
                # å–å¹³å‡å€¼æé«˜å‡†ç¡®æ€§
                processed['network_hashrate'] = (processed['network_hashrate'] + mempool_hashrate) / 2
        
        # åˆç†æ€§æ£€æŸ¥
        if processed['network_hashrate'] < 500 or processed['network_hashrate'] > 1500:
            logger.warning(f"å¼‚å¸¸ç®—åŠ›å€¼: {processed['network_hashrate']:.2f} EH/s")
            processed['network_hashrate'] = 900  # ä½¿ç”¨åˆç†é»˜è®¤å€¼
        
        if processed['network_difficulty'] <= 0:
            # åŸºäºç®—åŠ›ä¼°ç®—éš¾åº¦
            processed['network_difficulty'] = processed['network_hashrate'] * 1.4e14
        
        logger.info(f"ç½‘ç»œæ•°æ®å¤„ç†: ç®—åŠ›={processed['network_hashrate']:.2f}EH/s, éš¾åº¦={processed['network_difficulty']:.2e}")
        return processed
    
    def save_optimized_data(self, price_data: Dict, network_data: Dict):
        """ä¿å­˜ä¼˜åŒ–åçš„æ•°æ®"""
        try:
            conn = self.get_connection()
            cursor = conn.cursor()
            
            # æ’å…¥ä¸»æ•°æ®è¡¨
            insert_sql = """
                INSERT INTO market_analytics (
                    recorded_at, btc_price, btc_market_cap, btc_volume_24h,
                    network_hashrate, network_difficulty, 
                    price_change_24h, source, created_at
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            
            cursor.execute(insert_sql, (
                datetime.now(),
                price_data['consensus_price'],
                price_data.get('market_cap', 0),
                price_data.get('volume_24h', 0),
                network_data['network_hashrate'],
                network_data['network_difficulty'],
                price_data.get('price_change_24h', 0),
                f"optimized_v2_{len(price_data.get('sources', []))}sources",
                datetime.now()
            ))
            
            conn.commit()
            cursor.close()
            conn.close()
            
            # æ›´æ–°ç»Ÿè®¡
            self.collection_stats['successful_collections'] += 1
            self.collection_stats['last_collection'] = datetime.now()
            
            logger.info(f"ä¼˜åŒ–æ•°æ®å·²ä¿å­˜: ${price_data['consensus_price']:,.2f}, è´¨é‡åˆ†:{price_data.get('data_quality_score', 0):.1f}%")
            
        except Exception as e:
            self.collection_stats['failed_collections'] += 1
            logger.error(f"ä¼˜åŒ–æ•°æ®ä¿å­˜å¤±è´¥: {e}")
    
    def run_optimized_collection(self):
        """è¿è¡Œä¼˜åŒ–çš„æ•°æ®æ”¶é›†"""
        logger.info("å¼€å§‹ä¼˜åŒ–æ•°æ®æ”¶é›†...")
        self.collection_stats['total_collections'] += 1
        
        # å¹¶è¡Œæ”¶é›†ä»·æ ¼å’Œç½‘ç»œæ•°æ®
        with ThreadPoolExecutor(max_workers=2) as executor:
            price_future = executor.submit(self.collect_enhanced_price_data)
            network_future = executor.submit(self.collect_network_stats)
            
            try:
                price_data = price_future.result(timeout=20)
                network_data = network_future.result(timeout=20)
                
                if price_data and network_data:
                    self.save_optimized_data(price_data, network_data)
                    return True
                else:
                    logger.warning("ä»·æ ¼æˆ–ç½‘ç»œæ•°æ®æ”¶é›†å¤±è´¥")
                    return False
                    
            except Exception as e:
                logger.error(f"ä¼˜åŒ–æ•°æ®æ”¶é›†å¤±è´¥: {e}")
                return False
    
    def get_collection_statistics(self) -> Dict:
        """è·å–æ”¶é›†ç»Ÿè®¡ä¿¡æ¯"""
        success_rate = 0
        if self.collection_stats['total_collections'] > 0:
            success_rate = (self.collection_stats['successful_collections'] / 
                          self.collection_stats['total_collections']) * 100
        
        return {
            'total_collections': self.collection_stats['total_collections'],
            'successful_collections': self.collection_stats['successful_collections'],
            'failed_collections': self.collection_stats['failed_collections'],
            'success_rate': success_rate,
            'last_collection': self.collection_stats['last_collection']
        }
    
    def start_optimized_scheduler(self):
        """å¯åŠ¨ä¼˜åŒ–çš„è°ƒåº¦å™¨"""
        logger.info("å¯åŠ¨ä¼˜åŒ–æ•°æ®æ”¶é›†è°ƒåº¦å™¨...")
        
        # æ¯10åˆ†é’Ÿæ”¶é›†ä¸€æ¬¡ (æå‡åˆ°144æ¡/å¤©)
        schedule.every(10).minutes.do(self.run_optimized_collection)
        
        # è¿è¡Œè°ƒåº¦å™¨
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except KeyboardInterrupt:
                logger.info("è°ƒåº¦å™¨åœæ­¢")
                break
            except Exception as e:
                logger.error(f"è°ƒåº¦å™¨é”™è¯¯: {e}")
                time.sleep(60)

def main():
    """ä¸»å‡½æ•°"""
    processor = OptimizedBatchProcessor()
    
    logger.info("=== ä¼˜åŒ–æ‰¹é‡æ•°æ®å¤„ç†å™¨ ===")
    
    # è¿è¡Œå•æ¬¡æ”¶é›†æµ‹è¯•
    logger.info("è¿è¡Œä¼˜åŒ–æ•°æ®æ”¶é›†æµ‹è¯•...")
    success = processor.run_optimized_collection()
    
    if success:
        print("âœ… ä¼˜åŒ–æ•°æ®æ”¶é›†æµ‹è¯•æˆåŠŸï¼")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        stats = processor.get_collection_statistics()
        print(f"ğŸ“Š æ”¶é›†ç»Ÿè®¡ï¼š")
        print(f"   æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"   æ€»æ”¶é›†: {stats['total_collections']}")
        print(f"   æœ€åæ”¶é›†: {stats['last_collection']}")
        
        # è¯¢é—®æ˜¯å¦å¯åŠ¨æŒç»­è°ƒåº¦
        try:
            start_scheduler = input("æ˜¯å¦å¯åŠ¨æŒç»­æ•°æ®æ”¶é›†? (y/n): ").strip().lower()
            if start_scheduler == 'y':
                processor.start_optimized_scheduler()
        except KeyboardInterrupt:
            logger.info("ç”¨æˆ·å–æ¶ˆ")
    else:
        print("âŒ ä¼˜åŒ–æ•°æ®æ”¶é›†æµ‹è¯•å¤±è´¥")

# å¯¼å‡ºæ‰¹é‡å¤„ç†å™¨å‡½æ•°ä¾›å…¶ä»–æ¨¡å—ä½¿ç”¨
def batch_processor():
    """æ‰¹é‡æ•°æ®å¤„ç†å™¨å…¥å£å‡½æ•°"""
    processor = OptimizedBatchProcessor()
    return processor.run_optimized_collection()

if __name__ == "__main__":
    main()