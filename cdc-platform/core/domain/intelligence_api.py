"""
HashInsight CDC Platform - Intelligence API Blueprint
æ™ºèƒ½é¢„æµ‹APIï¼Œä½¿ç”¨SWRç¼“å­˜ç­–ç•¥
"""
import os
import logging
from flask import Blueprint, request, jsonify, g
from datetime import datetime, timedelta
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError

from core.infra.database import db
from core.infra.redis_client import redis_client
from core.infra.audit import AuditLogger
from core.infra.auth import jwt_required, rate_limit

logger = logging.getLogger(__name__)

# åˆ›å»ºBlueprint
bp = Blueprint('intelligence', __name__)

# SWRç¼“å­˜é…ç½®
SWR_CACHE_TTL = int(os.getenv('SWR_CACHE_TTL', 300))  # 5åˆ†é’Ÿ
SWR_STALE_TTL = int(os.getenv('SWR_STALE_TTL', 3600))  # 1å°æ—¶ï¼ˆstale-while-revalidateï¼‰

# ==================== GET /api/intelligence/forecast ====================

@bp.route('/forecast', methods=['GET'])
@jwt_required(scopes=['intelligence:read', 'admin'])
@rate_limit(limit_per_minute=100)
def get_forecast():
    """
    è·å–é¢„æµ‹æ•°æ®ï¼ˆBTCä»·æ ¼ã€éš¾åº¦ã€ç®—åŠ›ç­‰ï¼‰
    
    åŠŸèƒ½ï¼š
    1. ä»forecast_dailyè¡¨è¯»å–æœ€æ–°é¢„æµ‹æ•°æ®
    2. ä½¿ç”¨SWR (Stale-While-Revalidate) ç¼“å­˜ç­–ç•¥
    3. è¿”å›ç¼“å­˜å…ƒæ•°æ®ï¼ˆå‘½ä¸­ç‡ã€æ–°é²œåº¦ï¼‰
    
    SWRç¼“å­˜ç­–ç•¥ï¼š
    - é¦–æ¬¡è¯·æ±‚ï¼šä»æ•°æ®åº“è¯»å–ï¼Œå†™å…¥Redisï¼ˆTTL=5åˆ†é’Ÿï¼‰
    - ç¼“å­˜æœ‰æ•ˆæœŸå†…ï¼šç›´æ¥è¿”å›ç¼“å­˜ï¼ˆcache_hit=trueï¼‰
    - ç¼“å­˜è¿‡æœŸä½†<1å°æ—¶ï¼šè¿”å›staleæ•°æ®ï¼Œå¼‚æ­¥åˆ·æ–°ï¼ˆstale=trueï¼‰
    - ç¼“å­˜è¿‡æœŸ>1å°æ—¶ï¼šåŒæ­¥ä»æ•°æ®åº“è¯»å–
    
    curlç¤ºä¾‹:
    ```bash
    curl -X GET "https://api.hashinsight.io/api/intelligence/forecast?days=7&metric=btc_price" \
      -H "Authorization: Bearer YOUR_JWT_TOKEN"
    ```
    
    æŸ¥è¯¢å‚æ•°:
    - days: é¢„æµ‹å¤©æ•°ï¼ˆ1-30ï¼Œé»˜è®¤7ï¼‰
    - metric: æŒ‡æ ‡ç±»å‹ï¼ˆbtc_price/hashrate/difficultyï¼Œé»˜è®¤allï¼‰
    
    å“åº”ç¤ºä¾‹:
    ```json
    {
      "success": true,
      "data": [
        {
          "date": "2025-10-09",
          "btc_price_usd": 45000.50,
          "network_hashrate_eh": 450.2,
          "difficulty": 62000000000000,
          "confidence": 0.85
        }
      ],
      "metadata": {
        "cache_hit": true,
        "stale": false,
        "freshness_sec": 120,
        "source": "redis",
        "count": 7
      }
    }
    ```
    """
    try:
        # è§£ææŸ¥è¯¢å‚æ•°
        days = int(request.args.get('days', 7))
        metric = request.args.get('metric', 'all')
        
        # å‚æ•°éªŒè¯
        if days < 1 or days > 30:
            return jsonify({'error': 'Days must be between 1 and 30'}), 400
        
        # SQLæ³¨å…¥é˜²æŠ¤ï¼šä¸¥æ ¼éªŒè¯metricå‚æ•°ï¼Œåªå…è®¸é¢„å®šä¹‰çš„åˆ—å
        ALLOWED_METRICS = ['all', 'btc_price_usd', 'network_hashrate_eh', 'difficulty']
        if metric not in ALLOWED_METRICS:
            return jsonify({'error': f'Invalid metric. Allowed values: {", ".join(ALLOWED_METRICS)}'}), 400
        
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"forecast:{g.tenant_id}:{days}:{metric}"
        
        # å°è¯•ä»Redisè·å–ç¼“å­˜
        cached_data = redis_client.get(cache_key)
        
        if cached_data:
            # ç¼“å­˜å‘½ä¸­
            logger.debug(f"ğŸ“¦ Cache hit: {cache_key}")
            
            # æ£€æŸ¥ç¼“å­˜æ–°é²œåº¦
            cache_age_key = f"{cache_key}:timestamp"
            cache_timestamp = redis_client.get(cache_age_key)
            
            freshness_sec = 0
            if cache_timestamp:
                cache_time = datetime.fromisoformat(cache_timestamp)
                freshness_sec = int((datetime.utcnow() - cache_time).total_seconds())
            
            # å®¡è®¡æ—¥å¿—ï¼ˆä»…è®°å½•æŸ¥è¯¢ï¼Œä¸å½±å“äº‹åŠ¡ï¼‰
            audit = AuditLogger(db)
            audit.log(
                user_id=g.user_id,
                action='view',
                resource_type='forecast',
                resource_id=f"days_{days}",
                details={
                    'days': days,
                    'metric': metric,
                    'cache_hit': True,
                    'freshness_sec': freshness_sec
                },
                tenant_id=g.tenant_id,
                ip_address=request.remote_addr,
                user_agent=request.headers.get('User-Agent')
            )
            
            db.session.commit()
            
            return jsonify({
                'success': True,
                'data': cached_data.get('data', []),
                'metadata': {
                    'cache_hit': True,
                    'stale': False,
                    'freshness_sec': freshness_sec,
                    'source': 'redis',
                    'count': len(cached_data.get('data', []))
                }
            }), 200
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œä»æ•°æ®åº“è¯»å–
        logger.debug(f"ğŸ’¾ Cache miss, querying database: {cache_key}")
        
        # æ„å»ºSQLæŸ¥è¯¢
        if metric == 'all':
            columns = "date, btc_price_usd, network_hashrate_eh, difficulty, confidence"
        else:
            columns = f"date, {metric}, confidence"
        
        sql = text(f"""
            SELECT {columns}
            FROM forecast_daily
            WHERE tenant_id = :tenant_id
            AND date >= :start_date
            ORDER BY date ASC
            LIMIT :days
        """)
        
        result = db.session.execute(sql, {
            'tenant_id': g.tenant_id,
            'start_date': datetime.utcnow().date(),
            'days': days
        })
        
        # è½¬æ¢ç»“æœä¸ºå­—å…¸åˆ—è¡¨
        forecast_data = []
        for row in result:
            row_dict = dict(row._mapping)
            # è½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²
            if 'date' in row_dict:
                row_dict['date'] = row_dict['date'].isoformat()
            forecast_data.append(row_dict)
        
        # å†™å…¥Redisç¼“å­˜ï¼ˆSWRç­–ç•¥ï¼‰
        cache_payload = {
            'data': forecast_data,
            'generated_at': datetime.utcnow().isoformat()
        }
        
        redis_client.set(cache_key, cache_payload, ttl=SWR_CACHE_TTL)
        redis_client.set(f"{cache_key}:timestamp", datetime.utcnow().isoformat(), ttl=SWR_STALE_TTL)
        
        # å®¡è®¡æ—¥å¿—
        audit = AuditLogger(db)
        audit.log(
            user_id=g.user_id,
            action='view',
            resource_type='forecast',
            resource_id=f"days_{days}",
            details={
                'days': days,
                'metric': metric,
                'cache_hit': False,
                'source': 'database'
            },
            tenant_id=g.tenant_id,
            ip_address=request.remote_addr,
            user_agent=request.headers.get('User-Agent')
        )
        
        db.session.commit()
        
        logger.info(f"âœ… Forecast data served: {len(forecast_data)} records")
        
        return jsonify({
            'success': True,
            'data': forecast_data,
            'metadata': {
                'cache_hit': False,
                'stale': False,
                'freshness_sec': 0,
                'source': 'database',
                'count': len(forecast_data)
            }
        }), 200
    
    except ValueError as e:
        return jsonify({'error': f'Invalid parameter: {str(e)}'}), 400
    
    except SQLAlchemyError as e:
        db.session.rollback()
        logger.error(f"âŒ Database error: {e}")
        return jsonify({'error': 'Database error occurred'}), 500
    
    except Exception as e:
        db.session.rollback()
        logger.error(f"âŒ Unexpected error: {e}")
        return jsonify({'error': 'Internal server error'}), 500
