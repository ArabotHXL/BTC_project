"""
HashInsight CDC Platform - Kafka Consumer Common Module
Kafkaæ¶ˆè´¹è€…é€šç”¨åŸºç±»ã€é‡è¯•æœºåˆ¶ã€DLQå¤„ç†ã€å¹‚ç­‰æ€§ä¿è¯

Author: HashInsight Team
Version: 1.0.0
"""
import os
import sys
import json
import logging
import signal
import traceback
import time
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any, Callable
from datetime import datetime
from kafka import KafkaConsumer  # type: ignore
from kafka.errors import KafkaError  # type: ignore

# æ·»åŠ CDCå¹³å°æ ¸å¿ƒæ¨¡å—åˆ°Pythonè·¯å¾„
CDC_CORE_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'core'))
sys.path.insert(0, CDC_CORE_PATH)

from flask import Flask
from infra.database import db, init_db  # type: ignore
from infra.models import ConsumerInbox, EventDLQ  # type: ignore
from infra.redis_client import redis_client  # type: ignore

logger = logging.getLogger(__name__)

# =====================================================
# Kafkaæ¶ˆè´¹è€…åŸºç±»
# =====================================================

class KafkaConsumerBase(ABC):
    """
    Kafkaæ¶ˆè´¹è€…åŸºç±»
    
    æä¾›åŠŸèƒ½ï¼š
    - Kafkaè¿æ¥ç®¡ç†
    - Inboxå¹‚ç­‰æ€§ä¿è¯
    - åˆ†å¸ƒå¼é”ï¼ˆRedisï¼‰
    - DLQæ­»ä¿¡é˜Ÿåˆ—å¤„ç†
    - é‡è¯•æœºåˆ¶
    - ä¼˜é›…å…³é—­
    """
    
    def __init__(self, consumer_name: str, topic: str, group_id: str):
        """
        åˆå§‹åŒ–æ¶ˆè´¹è€…
        
        å‚æ•°:
            consumer_name: æ¶ˆè´¹è€…åç§°ï¼ˆå¦‚ 'portfolio-consumer'ï¼‰
            topic: Kafkaä¸»é¢˜åç§°ï¼ˆå¦‚ 'events.miner'ï¼‰
            group_id: Kafkaæ¶ˆè´¹è€…ç»„ID
        """
        self.consumer_name = consumer_name
        self.topic = topic
        self.group_id = group_id
        self.consumer = None
        self.running = False
        self.app = None
        
        # é…ç½®å‚æ•°
        self.kafka_bootstrap_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
        self.max_retries = int(os.getenv('CONSUMER_MAX_RETRIES', 3))
        self.processing_timeout = int(os.getenv('CONSUMER_PROCESSING_TIMEOUT', 300))  # 5åˆ†é’Ÿ
        
        # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆä¼˜é›…å…³é—­ï¼‰
        signal.signal(signal.SIGINT, self._shutdown_handler)
        signal.signal(signal.SIGTERM, self._shutdown_handler)
        
        logger.info(f"âœ… {self.consumer_name} initialized: topic={topic}, group={group_id}")
    
    def _init_flask_app(self):
        """åˆå§‹åŒ–Flaskåº”ç”¨ä¸Šä¸‹æ–‡ï¼ˆç”¨äºæ•°æ®åº“æ“ä½œï¼‰"""
        self.app = Flask(__name__)
        self.app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DATABASE_URL')
        self.app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False
        init_db(self.app)
        logger.info("âœ… Flask application context initialized")
    
    def _create_consumer(self):
        """åˆ›å»ºKafkaæ¶ˆè´¹è€…å®ä¾‹"""
        try:
            self.consumer = KafkaConsumer(
                self.topic,
                bootstrap_servers=self.kafka_bootstrap_servers,
                group_id=self.group_id,
                auto_offset_reset='earliest',  # ä»æœ€æ—©çš„æ¶ˆæ¯å¼€å§‹æ¶ˆè´¹
                enable_auto_commit=False,  # æ‰‹åŠ¨æäº¤offsetï¼ˆä¿è¯è‡³å°‘ä¸€æ¬¡æ¶ˆè´¹ï¼‰
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                key_deserializer=lambda k: k.decode('utf-8') if k else None,
                max_poll_interval_ms=300000,  # 5åˆ†é’Ÿ
                session_timeout_ms=30000,  # 30ç§’
                heartbeat_interval_ms=10000,  # 10ç§’
            )
            logger.info(f"âœ… Kafka consumer created: {self.kafka_bootstrap_servers}")
        except KafkaError as e:
            logger.error(f"âŒ Failed to create Kafka consumer: {e}")
            raise
    
    def _check_inbox(self, event_id: str) -> bool:
        """
        æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦å·²è¢«æ¶ˆè´¹ï¼ˆInboxå¹‚ç­‰æ€§ï¼‰
        
        å‚æ•°:
            event_id: äº‹ä»¶ID
        
        è¿”å›:
            True: å·²æ¶ˆè´¹ï¼ˆè·³è¿‡ï¼‰
            False: æœªæ¶ˆè´¹ï¼ˆå¤„ç†ï¼‰
        """
        try:
            if not self.app:
                return False
            with self.app.app_context():
                existing = ConsumerInbox.query.filter_by(
                    consumer_name=self.consumer_name,
                    event_id=event_id
                ).first()
                
                if existing:
                    logger.debug(f"â­ï¸ Event {event_id} already consumed, skipping (idempotent)")
                    return True
                
                return False
        except Exception as e:
            logger.error(f"âŒ Error checking inbox for event {event_id}: {e}")
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œä¸ºå®‰å…¨èµ·è§è¿”å›Falseï¼ˆå°è¯•å¤„ç†ï¼‰
            return False
    
    def _save_to_inbox(self, event_id: str, event_kind: str, user_id: str, 
                       payload: Dict, processing_duration_ms: int):
        """
        ä¿å­˜äº‹ä»¶åˆ°Inboxï¼ˆæ ‡è®°å·²æ¶ˆè´¹ï¼‰
        
        å‚æ•°:
            event_id: äº‹ä»¶ID
            event_kind: äº‹ä»¶ç±»å‹
            user_id: ç”¨æˆ·ID
            payload: äº‹ä»¶è´Ÿè½½
            processing_duration_ms: å¤„ç†è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        """
        try:
            if not self.app:
                return
            with self.app.app_context():
                inbox_entry = ConsumerInbox(
                    consumer_name=self.consumer_name,
                    event_id=event_id,
                    event_kind=event_kind,
                    user_id=user_id,
                    payload=payload,
                    processing_duration_ms=str(processing_duration_ms)
                )
                db.session.add(inbox_entry)
                db.session.commit()
                logger.debug(f"ğŸ“¥ Saved to inbox: event_id={event_id}, duration={processing_duration_ms}ms")
        except Exception as e:
            logger.error(f"âŒ Error saving to inbox: {e}")
            db.session.rollback()
    
    def _save_to_dlq(self, event_id: str, event_kind: str, payload: Dict, 
                     error_message: str, retry_count: int):
        """
        ä¿å­˜å¤±è´¥äº‹ä»¶åˆ°æ­»ä¿¡é˜Ÿåˆ—ï¼ˆDLQï¼‰
        
        å‚æ•°:
            event_id: äº‹ä»¶ID
            event_kind: äº‹ä»¶ç±»å‹
            payload: äº‹ä»¶è´Ÿè½½
            error_message: é”™è¯¯ä¿¡æ¯
            retry_count: é‡è¯•æ¬¡æ•°
        """
        try:
            if not self.app:
                return
            with self.app.app_context():
                dlq_entry = EventDLQ(
                    id=f"{event_id}_dlq_{int(time.time())}",
                    consumer_name=self.consumer_name,
                    event_id=event_id,
                    event_kind=event_kind,
                    payload=payload,
                    error_message=error_message,
                    error_stacktrace=traceback.format_exc(),
                    retry_count=str(retry_count)
                )
                db.session.add(dlq_entry)
                db.session.commit()
                logger.error(f"ğŸ’€ Saved to DLQ: event_id={event_id}, retries={retry_count}, error={error_message}")
        except Exception as e:
            logger.error(f"âŒ Error saving to DLQ: {e}")
            db.session.rollback()
    
    def _acquire_lock(self, lock_name: str) -> Optional[Any]:
        """
        è·å–åˆ†å¸ƒå¼é”ï¼ˆé˜²æ­¢å¹¶å‘å¤„ç†åŒä¸€ç”¨æˆ·çš„äº‹ä»¶ï¼‰
        
        å‚æ•°:
            lock_name: é”åç§°ï¼ˆå¦‚ 'recalc:123'ï¼‰
        
        è¿”å›:
            é”å¯¹è±¡æˆ–None
        """
        lock = redis_client.acquire_lock(
            lock_name=lock_name,
            timeout=self.processing_timeout,
            blocking_timeout=5
        )
        
        if lock:
            logger.debug(f"ğŸ”’ Acquired lock: {lock_name}")
        else:
            logger.warning(f"â³ Failed to acquire lock (already processing): {lock_name}")
        
        return lock
    
    def _release_lock(self, lock):
        """é‡Šæ”¾åˆ†å¸ƒå¼é”"""
        if lock:
            redis_client.release_lock(lock)
            logger.debug(f"ğŸ”“ Released lock")
    
    def _invalidate_cache(self, cache_key: str):
        """
        å¤±æ•ˆç¼“å­˜
        
        å‚æ•°:
            cache_key: ç¼“å­˜é”®ï¼ˆå¦‚ 'user_portfolio:123'ï¼‰
        """
        success = redis_client.delete(cache_key)
        if success:
            logger.debug(f"ğŸ—‘ï¸ Cache invalidated: {cache_key}")
        else:
            logger.warning(f"âš ï¸ Failed to invalidate cache: {cache_key}")
    
    def _process_with_retry(self, event_id: str, event_kind: str, user_id: str, 
                           payload: Dict, process_func: Callable) -> bool:
        """
        å¸¦é‡è¯•æœºåˆ¶çš„æ¶ˆæ¯å¤„ç†
        
        å‚æ•°:
            event_id: äº‹ä»¶ID
            event_kind: äº‹ä»¶ç±»å‹
            user_id: ç”¨æˆ·ID
            payload: äº‹ä»¶è´Ÿè½½
            process_func: å¤„ç†å‡½æ•°
        
        è¿”å›:
            True: å¤„ç†æˆåŠŸ
            False: å¤„ç†å¤±è´¥ï¼ˆå·²é‡è¯•max_retriesæ¬¡ï¼‰
        """
        start_time = time.time()
        retry_count = 0
        last_error = None
        
        while retry_count <= self.max_retries:
            try:
                # è°ƒç”¨å¤„ç†å‡½æ•°
                process_func(event_id, event_kind, user_id, payload)
                
                # å¤„ç†æˆåŠŸï¼šè®¡ç®—è€—æ—¶å¹¶ä¿å­˜åˆ°Inbox
                duration_ms = int((time.time() - start_time) * 1000)
                self._save_to_inbox(event_id, event_kind, user_id, payload, duration_ms)
                
                logger.info(f"âœ… Event processed successfully: {event_id} (duration={duration_ms}ms)")
                return True
                
            except Exception as e:
                last_error = str(e)
                retry_count += 1
                
                if retry_count <= self.max_retries:
                    # æŒ‡æ•°é€€é¿é‡è¯•
                    wait_time = min(2 ** retry_count, 30)  # æœ€å¤šç­‰å¾…30ç§’
                    logger.warning(
                        f"âš ï¸ Event processing failed (retry {retry_count}/{self.max_retries}): "
                        f"{event_id}, error={last_error}, retrying in {wait_time}s..."
                    )
                    time.sleep(wait_time)
                else:
                    # é‡è¯•æ¬¡æ•°è€—å°½ï¼šä¿å­˜åˆ°DLQ
                    logger.error(
                        f"âŒ Event processing failed after {self.max_retries} retries: "
                        f"{event_id}, error={last_error}"
                    )
                    self._save_to_dlq(event_id, event_kind, payload, last_error, retry_count - 1)
                    return False
        
        return False
    
    @abstractmethod
    def process_event(self, event_id: str, event_kind: str, user_id: str, payload: Dict):
        """
        å¤„ç†å•ä¸ªäº‹ä»¶ï¼ˆå­ç±»å¿…é¡»å®ç°ï¼‰
        
        å‚æ•°:
            event_id: äº‹ä»¶ID
            event_kind: äº‹ä»¶ç±»å‹
            user_id: ç”¨æˆ·ID
            payload: äº‹ä»¶è´Ÿè½½
        """
        pass
    
    def run(self):
        """è¿è¡Œæ¶ˆè´¹è€…ä¸»å¾ªç¯"""
        logger.info(f"ğŸš€ Starting {self.consumer_name}...")
        
        # åˆå§‹åŒ–Flaskåº”ç”¨ä¸Šä¸‹æ–‡
        self._init_flask_app()
        
        # åˆ›å»ºKafkaæ¶ˆè´¹è€…
        self._create_consumer()
        
        self.running = True
        logger.info(f"âœ… {self.consumer_name} is running, listening to topic '{self.topic}'")
        
        try:
            if not self.consumer:
                logger.error("âŒ Kafka consumer not initialized")
                return
            for message in self.consumer:
                if not self.running:
                    break
                
                try:
                    # è§£ææ¶ˆæ¯
                    event_id = message.value.get('id')
                    event_kind = message.value.get('kind')
                    user_id = message.key or message.value.get('user_id')
                    payload = message.value.get('payload', message.value)
                    
                    if not event_id or not user_id:
                        logger.error(f"âŒ Invalid message format (missing id/user_id): {message.value}")
                        if self.consumer:
                            self.consumer.commit()
                        continue
                    
                    logger.info(f"ğŸ“¨ Received event: id={event_id}, kind={event_kind}, user={user_id}")
                    
                    # æ£€æŸ¥æ˜¯å¦å·²æ¶ˆè´¹ï¼ˆå¹‚ç­‰æ€§ï¼‰
                    if self._check_inbox(event_id):
                        if self.consumer:
                            self.consumer.commit()
                        continue
                    
                    # å¸¦é‡è¯•æœºåˆ¶çš„å¤„ç†
                    success = self._process_with_retry(
                        event_id, event_kind, user_id, payload, self.process_event
                    )
                    
                    # æäº¤offsetï¼ˆæ— è®ºæˆåŠŸæˆ–å¤±è´¥éƒ½æäº¤ï¼Œé¿å…é‡å¤æ¶ˆè´¹ï¼‰
                    if self.consumer:
                        self.consumer.commit()
                    
                    if success:
                        logger.info(f"âœ… Message committed: {event_id}")
                    else:
                        logger.error(f"âŒ Message failed and sent to DLQ: {event_id}")
                
                except Exception as e:
                    logger.error(f"âŒ Error processing message: {e}\n{traceback.format_exc()}")
                    # å³ä½¿å‡ºé”™ä¹Ÿæäº¤offsetï¼Œé¿å…æ— é™é‡è¯•
                    if self.consumer:
                        self.consumer.commit()
        
        except KeyboardInterrupt:
            logger.info("â¹ï¸ Consumer interrupted by user")
        
        finally:
            self._cleanup()
    
    def _cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info(f"ğŸ§¹ Cleaning up {self.consumer_name}...")
        
        if self.consumer:
            self.consumer.close()
            logger.info("âœ… Kafka consumer closed")
        
        logger.info(f"âœ… {self.consumer_name} stopped gracefully")
    
    def _shutdown_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨ï¼ˆä¼˜é›…å…³é—­ï¼‰"""
        logger.info(f"ğŸ›‘ Received shutdown signal ({signum}), stopping consumer...")
        self.running = False


# =====================================================
# å·¥å…·å‡½æ•°
# =====================================================

def format_error_message(error: Exception) -> str:
    """
    æ ¼å¼åŒ–é”™è¯¯æ¶ˆæ¯
    
    å‚æ•°:
        error: å¼‚å¸¸å¯¹è±¡
    
    è¿”å›:
        æ ¼å¼åŒ–çš„é”™è¯¯æ¶ˆæ¯
    """
    return f"{type(error).__name__}: {str(error)}"


def extract_user_id(message: Dict) -> Optional[str]:
    """
    ä»æ¶ˆæ¯ä¸­æå–user_id
    
    å‚æ•°:
        message: Kafkaæ¶ˆæ¯
    
    è¿”å›:
        ç”¨æˆ·IDæˆ–None
    """
    return message.get('user_id') or message.get('payload', {}).get('user_id')


def validate_event_payload(payload: Dict, required_fields: list) -> bool:
    """
    éªŒè¯äº‹ä»¶è´Ÿè½½æ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µ
    
    å‚æ•°:
        payload: äº‹ä»¶è´Ÿè½½
        required_fields: å¿…éœ€å­—æ®µåˆ—è¡¨
    
    è¿”å›:
        True: éªŒè¯é€šè¿‡
        False: éªŒè¯å¤±è´¥
    """
    for field in required_fields:
        if field not in payload:
            logger.error(f"âŒ Missing required field: {field}")
            return False
    return True
