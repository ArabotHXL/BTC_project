#!/usr/bin/env python3
"""
æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–è„šæœ¬
Database Index Optimization Script

ä¸ºå…³é”®è¡¨æ·»åŠ ç´¢å¼•ä»¥æå‡æŸ¥è¯¢æ€§èƒ½
Add indexes to critical tables for performance improvement

ç›®æ ‡ï¼š
- æŸ¥è¯¢å“åº”æ—¶é—´ p95 â‰¤250ms
- æ”¯æŒ5000+çŸ¿æœºé«˜æ•ˆæŸ¥è¯¢
- ä¼˜åŒ–å†å²æ•°æ®æŸ¥è¯¢
- æå‡æŠ¥è¡¨ç”Ÿæˆé€Ÿåº¦

Targets:
- Query response time p95 â‰¤250ms
- Efficient queries for 5000+ miners
- Optimized historical data queries
- Faster report generation
"""

import logging
import os
import sys
import re
from sqlalchemy import create_engine, text, inspect
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DatabaseOptimizer:
    """æ•°æ®åº“ä¼˜åŒ–å™¨"""
    
    def __init__(self, database_url: str = None):
        """
        åˆå§‹åŒ–æ•°æ®åº“ä¼˜åŒ–å™¨
        
        Parameters:
        -----------
        database_url : str
            æ•°æ®åº“è¿æ¥URL
        """
        self.database_url = database_url or os.environ.get('DATABASE_URL')
        if not self.database_url:
            raise ValueError("DATABASE_URL environment variable must be set")
        
        self.engine = create_engine(self.database_url)
        self.inspector = inspect(self.engine)
        logger.info(f"âœ… å·²è¿æ¥åˆ°æ•°æ®åº“")
    
    def get_existing_indexes(self, table_name: str) -> list:
        """è·å–è¡¨çš„ç°æœ‰ç´¢å¼•"""
        try:
            indexes = self.inspector.get_indexes(table_name)
            return [idx['name'] for idx in indexes]
        except Exception as e:
            logger.warning(f"è·å–è¡¨ {table_name} ç´¢å¼•å¤±è´¥: {e}")
            return []
    
    def create_index_if_not_exists(self, table_name: str, index_name: str, 
                                   columns: list, unique: bool = False):
        """
        åˆ›å»ºç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        
        Parameters:
        -----------
        table_name : str
            è¡¨å
        index_name : str
            ç´¢å¼•å
        columns : list
            åˆ—ååˆ—è¡¨
        unique : bool
            æ˜¯å¦å”¯ä¸€ç´¢å¼•
        """
        try:
            existing_indexes = self.get_existing_indexes(table_name)
            
            if index_name in existing_indexes:
                logger.info(f"â­ï¸  ç´¢å¼•å·²å­˜åœ¨: {index_name} on {table_name}")
                return False
            
            identifier_re = re.compile(r'^[A-Za-z_][A-Za-z0-9_]*$')
            for ident in [table_name, index_name] + list(columns):
                if not identifier_re.match(ident):
                    raise ValueError(f"Invalid SQL identifier: {ident}")
            
            columns_str = ', '.join(columns)
            unique_str = 'UNIQUE ' if unique else ''
            sql = f"CREATE {unique_str}INDEX {index_name} ON {table_name} ({columns_str})"
            
            with self.engine.connect() as conn:
                conn.execute(text(sql))
                conn.commit()
            
            logger.info(f"âœ… åˆ›å»ºç´¢å¼•æˆåŠŸ: {index_name} on {table_name}({columns_str})")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºç´¢å¼•å¤±è´¥ {index_name}: {e}")
            return False
    
    def optimize_user_miner_table(self):
        """ä¼˜åŒ– user_miner è¡¨ï¼ˆç”¨æˆ·çŸ¿æœºæ•°æ®ï¼‰"""
        logger.info("ğŸ”§ ä¼˜åŒ– user_miner è¡¨...")
        
        # ç”¨æˆ·IDç´¢å¼•ï¼ˆæœ€å¸¸ç”¨çš„æŸ¥è¯¢æ¡ä»¶ï¼‰
        self.create_index_if_not_exists(
            'user_miner', 'idx_user_miner_user_id', ['user_id']
        )
        
        # åˆ›å»ºæ—¶é—´ç´¢å¼•ï¼ˆç”¨äºæ—¶é—´èŒƒå›´æŸ¥è¯¢ï¼‰
        self.create_index_if_not_exists(
            'user_miner', 'idx_user_miner_created_at', ['created_at']
        )
        
        # å¤åˆç´¢å¼•ï¼šuser_id + created_atï¼ˆç”¨äºç”¨æˆ·å†å²æŸ¥è¯¢ï¼‰
        self.create_index_if_not_exists(
            'user_miner', 'idx_user_miner_user_created', ['user_id', 'created_at']
        )
        
        # çŸ¿æœºå‹å·ç´¢å¼•ï¼ˆç”¨äºæŒ‰å‹å·ç­›é€‰ï¼‰
        self.create_index_if_not_exists(
            'user_miner', 'idx_user_miner_model', ['model']
        )
    
    def optimize_network_snapshot_table(self):
        """ä¼˜åŒ– network_snapshot è¡¨ï¼ˆç½‘ç»œå¿«ç…§æ•°æ®ï¼‰"""
        logger.info("ğŸ”§ ä¼˜åŒ– network_snapshot è¡¨...")
        
        # æ—¶é—´æˆ³ç´¢å¼•ï¼ˆç”¨äºå†å²æ•°æ®æŸ¥è¯¢ï¼‰
        self.create_index_if_not_exists(
            'network_snapshot', 'idx_network_snapshot_timestamp', ['timestamp']
        )
        
        # æ•°æ®æ¥æºç´¢å¼•ï¼ˆç”¨äºæŒ‰æ¥æºç­›é€‰ï¼‰
        self.create_index_if_not_exists(
            'network_snapshot', 'idx_network_snapshot_source', ['source']
        )
        
        # å¤åˆç´¢å¼•ï¼štimestamp + sourceï¼ˆç”¨äºæ•°æ®å¯¹æ¯”ï¼‰
        self.create_index_if_not_exists(
            'network_snapshot', 'idx_network_snapshot_ts_source', ['timestamp', 'source']
        )
    
    def optimize_miner_telemetry_table(self):
        """ä¼˜åŒ– miner_telemetry è¡¨ï¼ˆçŸ¿æœºé¥æµ‹æ•°æ®ï¼‰"""
        logger.info("ğŸ”§ ä¼˜åŒ– miner_telemetry è¡¨...")
        
        # çŸ¿æœºIDç´¢å¼•
        self.create_index_if_not_exists(
            'miner_telemetry', 'idx_miner_telemetry_miner_id', ['miner_id']
        )
        
        # æ—¶é—´æˆ³ç´¢å¼•ï¼ˆç”¨äºæ—¶é—´åºåˆ—æŸ¥è¯¢ï¼‰
        self.create_index_if_not_exists(
            'miner_telemetry', 'idx_miner_telemetry_timestamp', ['timestamp']
        )
        
        # å¤åˆç´¢å¼•ï¼šminer_id + timestampï¼ˆç”¨äºå•çŸ¿æœºå†å²æŸ¥è¯¢ï¼‰
        self.create_index_if_not_exists(
            'miner_telemetry', 'idx_miner_telemetry_miner_ts', ['miner_id', 'timestamp']
        )
        
        # çŠ¶æ€ç´¢å¼•ï¼ˆç”¨äºå‘Šè­¦æŸ¥è¯¢ï¼‰
        self.create_index_if_not_exists(
            'miner_telemetry', 'idx_miner_telemetry_status', ['status']
        )
    
    def optimize_blockchain_record_table(self):
        """ä¼˜åŒ– blockchain_record è¡¨ï¼ˆåŒºå—é“¾è®°å½•ï¼‰"""
        logger.info("ğŸ”§ ä¼˜åŒ– blockchain_record è¡¨...")
        
        # äº¤æ˜“å“ˆå¸Œå”¯ä¸€ç´¢å¼•
        self.create_index_if_not_exists(
            'blockchain_record', 'idx_blockchain_record_tx_hash', ['tx_hash'], unique=True
        )
        
        # çŠ¶æ€ç´¢å¼•ï¼ˆç”¨äºæŸ¥è¯¢å¾…å¤„ç†/å¤±è´¥çš„è®°å½•ï¼‰
        self.create_index_if_not_exists(
            'blockchain_record', 'idx_blockchain_record_status', ['status']
        )
        
        # åˆ›å»ºæ—¶é—´ç´¢å¼•
        self.create_index_if_not_exists(
            'blockchain_record', 'idx_blockchain_record_created', ['created_at']
        )
        
        # ç”¨æˆ·IDç´¢å¼•
        self.create_index_if_not_exists(
            'blockchain_record', 'idx_blockchain_record_user_id', ['user_id']
        )
    
    def optimize_hosting_miner_table(self):
        """ä¼˜åŒ– hosting_miner è¡¨ï¼ˆæ‰˜ç®¡çŸ¿æœºï¼‰"""
        logger.info("ğŸ”§ ä¼˜åŒ– hosting_miner è¡¨...")
        
        # ç«™ç‚¹IDç´¢å¼•
        self.create_index_if_not_exists(
            'hosting_miner', 'idx_hosting_miner_site_id', ['site_id']
        )
        
        # çŠ¶æ€ç´¢å¼•
        self.create_index_if_not_exists(
            'hosting_miner', 'idx_hosting_miner_status', ['status']
        )
        
        # å®¢æˆ·IDç´¢å¼•
        self.create_index_if_not_exists(
            'hosting_miner', 'idx_hosting_miner_customer_id', ['customer_id']
        )
        
        # å¤åˆç´¢å¼•ï¼šsite_id + statusï¼ˆç”¨äºç«™ç‚¹ç›‘æ§ï¼‰
        self.create_index_if_not_exists(
            'hosting_miner', 'idx_hosting_miner_site_status', ['site_id', 'status']
        )
    
    def optimize_login_record_table(self):
        """ä¼˜åŒ– login_record è¡¨ï¼ˆç™»å½•è®°å½•ï¼‰"""
        logger.info("ğŸ”§ ä¼˜åŒ– login_record è¡¨...")
        
        # ç”¨æˆ·IDç´¢å¼•
        self.create_index_if_not_exists(
            'login_record', 'idx_login_record_user_id', ['user_id']
        )
        
        # æ—¶é—´æˆ³ç´¢å¼•
        self.create_index_if_not_exists(
            'login_record', 'idx_login_record_timestamp', ['login_time']
        )
        
        # å¤åˆç´¢å¼•ï¼šuser_id + timestampï¼ˆç”¨äºç”¨æˆ·ç™»å½•å†å²ï¼‰
        self.create_index_if_not_exists(
            'login_record', 'idx_login_record_user_time', ['user_id', 'login_time']
        )
    
    def optimize_user_access_table(self):
        """ä¼˜åŒ– user_access è¡¨ï¼ˆç”¨æˆ·è®¿é—®è®°å½•ï¼‰"""
        logger.info("ğŸ”§ ä¼˜åŒ– user_access è¡¨...")
        
        # ç”¨æˆ·IDç´¢å¼•
        self.create_index_if_not_exists(
            'user_access', 'idx_user_access_user_id', ['user_id']
        )
        
        # è®¿é—®æ—¶é—´ç´¢å¼•
        self.create_index_if_not_exists(
            'user_access', 'idx_user_access_timestamp', ['access_time']
        )
        
        # ç«¯ç‚¹ç´¢å¼•ï¼ˆç”¨äºAPIä½¿ç”¨åˆ†æï¼‰
        self.create_index_if_not_exists(
            'user_access', 'idx_user_access_endpoint', ['endpoint']
        )
    
    def optimize_sla_metrics_table(self):
        """ä¼˜åŒ– sla_metrics è¡¨ï¼ˆSLAæŒ‡æ ‡ï¼‰"""
        logger.info("ğŸ”§ ä¼˜åŒ– sla_metrics è¡¨...")
        
        # æ—¶é—´æˆ³ç´¢å¼•
        self.create_index_if_not_exists(
            'sla_metrics', 'idx_sla_metrics_timestamp', ['recorded_at']
        )
        
        # ç«™ç‚¹IDç´¢å¼•
        self.create_index_if_not_exists(
            'sla_metrics', 'idx_sla_metrics_site_id', ['site_id']
        )
        
        # å¤åˆç´¢å¼•ï¼šsite_id + timestamp
        self.create_index_if_not_exists(
            'sla_metrics', 'idx_sla_metrics_site_time', ['site_id', 'recorded_at']
        )
    
    def analyze_table_stats(self, table_name: str):
        """åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯"""
        try:
            with self.engine.connect() as conn:
                # è·å–è¡¨å¤§å°
                result = conn.execute(text(f"""
                    SELECT 
                        pg_size_pretty(pg_total_relation_size('{table_name}')) as total_size,
                        pg_size_pretty(pg_relation_size('{table_name}')) as table_size,
                        pg_size_pretty(pg_indexes_size('{table_name}')) as indexes_size
                """))
                row = result.fetchone()
                
                if row:
                    logger.info(f"ğŸ“Š {table_name} - æ€»å¤§å°: {row[0]}, è¡¨å¤§å°: {row[1]}, ç´¢å¼•å¤§å°: {row[2]}")
        except Exception as e:
            logger.warning(f"åˆ†æè¡¨ {table_name} å¤±è´¥: {e}")
    
    def vacuum_analyze_all(self):
        """å¯¹æ‰€æœ‰è¡¨æ‰§è¡ŒVACUUM ANALYZE"""
        logger.info("ğŸ§¹ æ‰§è¡ŒVACUUM ANALYZE...")
        
        critical_tables = [
            'user_miner', 'network_snapshot', 'miner_telemetry',
            'blockchain_record', 'hosting_miner', 'login_record',
            'user_access', 'sla_metrics'
        ]
        
        try:
            # éœ€è¦autocommitæ¨¡å¼æ¥æ‰§è¡ŒVACUUM
            with self.engine.connect().execution_options(isolation_level="AUTOCOMMIT") as conn:
                for table in critical_tables:
                    try:
                        conn.execute(text(f"VACUUM ANALYZE {table}"))
                        logger.info(f"âœ… VACUUM ANALYZE {table} å®Œæˆ")
                    except Exception as e:
                        logger.warning(f"âš ï¸  VACUUM ANALYZE {table} å¤±è´¥: {e}")
        except Exception as e:
            logger.error(f"âŒ VACUUM ANALYZE æ‰§è¡Œå¤±è´¥: {e}")
    
    def optimize_all(self):
        """æ‰§è¡Œæ‰€æœ‰ä¼˜åŒ–"""
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹æ•°æ®åº“ä¼˜åŒ–")
        logger.info("=" * 60)
        
        start_time = datetime.now()
        
        # ä¼˜åŒ–å„ä¸ªè¡¨
        self.optimize_user_miner_table()
        self.optimize_network_snapshot_table()
        self.optimize_miner_telemetry_table()
        self.optimize_blockchain_record_table()
        self.optimize_hosting_miner_table()
        self.optimize_login_record_table()
        self.optimize_user_access_table()
        self.optimize_sla_metrics_table()
        
        # åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š è¡¨ç»Ÿè®¡ä¿¡æ¯")
        logger.info("=" * 60)
        for table in ['user_miner', 'network_snapshot', 'miner_telemetry', 
                      'blockchain_record', 'hosting_miner']:
            self.analyze_table_stats(table)
        
        # æ‰§è¡ŒVACUUM ANALYZE
        logger.info("\n" + "=" * 60)
        self.vacuum_analyze_all()
        
        elapsed = (datetime.now() - start_time).total_seconds()
        
        logger.info("\n" + "=" * 60)
        logger.info(f"âœ… æ•°æ®åº“ä¼˜åŒ–å®Œæˆï¼è€—æ—¶: {elapsed:.2f}ç§’")
        logger.info("=" * 60)
    
    def generate_optimization_report(self):
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ ä¼˜åŒ–å»ºè®®æŠ¥å‘Š")
        logger.info("=" * 60)
        
        recommendations = [
            "1. å®šæœŸæ‰§è¡Œ VACUUM ANALYZE ä»¥æ›´æ–°ç»Ÿè®¡ä¿¡æ¯",
            "2. ç›‘æ§æ…¢æŸ¥è¯¢æ—¥å¿—ï¼Œè¯†åˆ«éœ€è¦ä¼˜åŒ–çš„æŸ¥è¯¢",
            "3. è€ƒè™‘å¯¹å¤§è¡¨è¿›è¡Œåˆ†åŒºï¼ˆå¦‚æŒ‰æ—¶é—´åˆ†åŒºï¼‰",
            "4. å®šæœŸæ¸…ç†å†å²æ•°æ®ï¼Œä¿æŒè¡¨å¤§å°åˆç†",
            "5. ä½¿ç”¨è¿æ¥æ± ä¼˜åŒ–æ•°æ®åº“è¿æ¥",
            "6. è€ƒè™‘ä½¿ç”¨åªè¯»å‰¯æœ¬åˆ†æ‹…æŸ¥è¯¢è´Ÿè½½",
            "7. é…ç½®åˆé€‚çš„ work_mem å’Œ shared_buffers",
            "8. å¯ç”¨ pg_stat_statements æ‰©å±•ç›‘æ§æŸ¥è¯¢æ€§èƒ½"
        ]
        
        for rec in recommendations:
            logger.info(rec)


def main():
    """ä¸»å‡½æ•°"""
    try:
        optimizer = DatabaseOptimizer()
        optimizer.optimize_all()
        optimizer.generate_optimization_report()
        
        logger.info("\nğŸ‰ æ‰€æœ‰ä¼˜åŒ–ä»»åŠ¡å®Œæˆ!")
        return 0
    
    except Exception as e:
        logger.error(f"âŒ ä¼˜åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
