#!/usr/bin/env python3
"""
åŒºå—é“¾æ•°æ®è‡ªåŠ¨ä¸Šé“¾è°ƒåº¦å™¨
Blockchain Data Auto-Recording Scheduler

è¿™ä¸ªæ¨¡å—æä¾›å®šæ—¶ä»»åŠ¡åŠŸèƒ½ï¼Œå®šæœŸå°†æŒ–çŸ¿æ•°æ®è®°å½•åˆ°åŒºå—é“¾å’ŒIPFS
This module provides scheduled task functionality to periodically record mining data to blockchain and IPFS

ç‰¹æ€§ Features:
- å®šæ—¶ä»»åŠ¡è°ƒåº¦ Scheduled task management
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶ Error handling and retry mechanisms  
- æ‰¹é‡æ•°æ®å¤„ç† Batch data processing
- æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ Data integrity checks
- æ€§èƒ½ç›‘æ§ Performance monitoring
"""

import logging
import threading
import time
import schedule
import os
import psutil
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import json
import traceback
from sqlalchemy import text, create_engine
from sqlalchemy.exc import OperationalError

# å¯¼å…¥é¡¹ç›®æ¨¡å— Import project modules
from app import app, db
from models import BlockchainRecord, BlockchainVerificationStatus, SchedulerLock
try:
    from blockchain_integration import (
        quick_register_mining_data, 
        quick_verify_mining_data,
        get_blockchain_integration
    )
    BLOCKCHAIN_AVAILABLE = True
except ImportError:
    BLOCKCHAIN_AVAILABLE = False
    
try:
    from api_client import api_client
    API_CLIENT_AVAILABLE = True
except ImportError:
    API_CLIENT_AVAILABLE = False
    
try:
    from mining_calculator import calculate_mining_profitability
    MINING_CALC_AVAILABLE = True
except ImportError:
    MINING_CALC_AVAILABLE = False

# é…ç½®æ—¥å¿— Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class BlockchainScheduler:
    """åŒºå—é“¾æ•°æ®è‡ªåŠ¨è°ƒåº¦å™¨ Blockchain Data Auto Scheduler"""
    
    def __init__(self):
        self.is_running = False
        self.scheduler_thread = None
        self.leader_lock_acquired = False
        self.process_id = os.getpid()
        self.lock_key = 'blockchain_scheduler_leader'
        self.lock_timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶
        self.hostname = os.uname().nodename
        self.stats = {
            'tasks_executed': 0,
            'successful_records': 0,
            'failed_records': 0,
            'last_execution': None,
            'uptime_start': datetime.utcnow()
        }
        
        # ä»»åŠ¡é…ç½® Task configuration
        self.task_config = {
            'daily_summary_recording': {
                'enabled': True,
                'frequency': 'daily',  # daily, hourly, weekly
                'time': '02:00',  # 02:00 UTC
                'retry_attempts': 3,
                'retry_delay': 300  # 5 minutes
            },
            'network_state_recording': {
                'enabled': True,
                'frequency': 'hourly',
                'retry_attempts': 2,
                'retry_delay': 180  # 3 minutes
            },
            'batch_historical_sync': {
                'enabled': False,  # æ‰‹åŠ¨å¯ç”¨ Manual enable
                'frequency': 'weekly',
                'day': 'sunday',
                'time': '01:00',
                'retry_attempts': 5,
                'retry_delay': 600  # 10 minutes
            },
            'data_integrity_check': {
                'enabled': True,
                'frequency': 'daily',
                'time': '03:00',
                'retry_attempts': 2,
                'retry_delay': 300
            }
        }
        
        logger.info("åŒºå—é“¾è°ƒåº¦å™¨å·²åˆå§‹åŒ– Blockchain Scheduler initialized")
        
        # ğŸ”§ CRITICAL FIX: ä½¿ç”¨SchedulerLockæ¨¡å‹éªŒè¯å•å®ä¾‹æœºåˆ¶
        self._verify_scheduler_lock_integration()
    
    def _verify_scheduler_lock_integration(self):
        """
        ğŸ”§ CRITICAL FIX: éªŒè¯SchedulerLockæ¨¡å‹é›†æˆ
        Verify SchedulerLock model integration
        """
        try:
            with app.app_context():
                # éªŒè¯SchedulerLockè¡¨æ˜¯å¦å­˜åœ¨
                db.create_all()  # ç¡®ä¿è¡¨å·²åˆ›å»º
                
                # æµ‹è¯•SchedulerLockåŠŸèƒ½
                test_result = self._test_scheduler_lock_functionality()
                if test_result:
                    logger.info("âœ… SchedulerLockæ¨¡å‹é›†æˆéªŒè¯æˆåŠŸ SchedulerLock model integration verified")
                else:
                    logger.error("âŒ SchedulerLockæ¨¡å‹é›†æˆéªŒè¯å¤±è´¥ SchedulerLock model integration failed")
                    raise Exception("SchedulerLock integration test failed")
                    
        except Exception as e:
            logger.error(f"SchedulerLocké›†æˆéªŒè¯å¤±è´¥ SchedulerLock integration verification failed: {e}")
            raise
            
    def _test_scheduler_lock_functionality(self) -> bool:
        """æµ‹è¯•SchedulerLockåŠŸèƒ½"""
        try:
            test_key = f"test_lock_{self.process_id}"
            
            # æµ‹è¯•è·å–é”
            success = SchedulerLock.acquire_lock(
                lock_key=test_key,
                process_id=self.process_id,
                hostname=self.hostname,
                timeout_seconds=60,
                worker_info=f"test_worker_{self.process_id}"
            )
            
            if not success:
                logger.warning("âš ï¸ æµ‹è¯•é”è·å–å¤±è´¥ï¼Œå¯èƒ½å·²è¢«å…¶ä»–è¿›ç¨‹æŒæœ‰")
                return True  # è¿™æ˜¯é¢„æœŸè¡Œä¸º
            
            # æµ‹è¯•é”çŠ¶æ€æŸ¥è¯¢
            active_lock = SchedulerLock.get_active_lock(test_key)
            if not active_lock:
                logger.error("âŒ è·å–æ´»è·ƒé”å¤±è´¥")
                return False
                
            # æµ‹è¯•é”é‡Šæ”¾
            release_success = SchedulerLock.release_lock(test_key, self.process_id)
            if not release_success:
                logger.error("âŒ é‡Šæ”¾æµ‹è¯•é”å¤±è´¥")
                return False
                
            logger.info(f"âœ… SchedulerLockåŠŸèƒ½æµ‹è¯•é€šè¿‡: {active_lock.to_dict()}")
            return True
            
        except Exception as e:
            logger.error(f"SchedulerLockåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def _acquire_leader_lock(self) -> bool:
        """
        ğŸ”§ CRITICAL FIX: è·å–leaderé”ï¼ˆä½¿ç”¨SchedulerLockæ¨¡å‹ï¼‰
        Acquire leader lock using SchedulerLock model
        """
        try:
            with app.app_context():
                # ç”Ÿæˆworkerä¿¡æ¯
                worker_info = json.dumps({
                    'pid': self.process_id,
                    'hostname': self.hostname,
                    'start_time': datetime.utcnow().isoformat(),
                    'scheduler_type': 'BlockchainScheduler'
                })
                
                # ä½¿ç”¨SchedulerLockæ¨¡å‹è·å–é”
                success = SchedulerLock.acquire_lock(
                    lock_key=self.lock_key,
                    process_id=self.process_id,
                    hostname=self.hostname,
                    timeout_seconds=self.lock_timeout,
                    worker_info=worker_info
                )
                
                if success:
                    self.leader_lock_acquired = True
                    logger.info(f"ğŸ”’ Leaderé”å·²è·å– Leader lock acquired: PID={self.process_id}@{self.hostname}")
                    
                    # è®°å½•é”çŠ¶æ€ç”¨äºç›‘æ§
                    active_lock = SchedulerLock.get_active_lock(self.lock_key)
                    if active_lock:
                        logger.info(f"ğŸ” é”çŠ¶æ€è¯¦æƒ…: {active_lock.to_dict()}")
                    
                    return True
                else:
                    # æ£€æŸ¥ç°æœ‰é”çŠ¶æ€
                    existing_lock = SchedulerLock.get_active_lock(self.lock_key)
                    if existing_lock:
                        logger.info(f"â³ Leaderé”è¢«å…¶ä»–è¿›ç¨‹æŒæœ‰: {existing_lock.to_dict()}")
                    return False
                    
        except Exception as e:
            logger.error(f"è·å–leaderé”å¤±è´¥ Failed to acquire leader lock: {e}")
            return False
    
    def _release_leader_lock(self):
        """
        ğŸ”§ CRITICAL FIX: é‡Šæ”¾leaderé”ï¼ˆä½¿ç”¨SchedulerLockæ¨¡å‹ï¼‰
        Release leader lock using SchedulerLock model
        """
        if not self.leader_lock_acquired:
            return
            
        try:
            with app.app_context():
                success = SchedulerLock.release_lock(
                    lock_key=self.lock_key,
                    process_id=self.process_id
                )
                
                if success:
                    logger.info(f"ğŸ”“ Leaderé”å·²é‡Šæ”¾ Leader lock released: PID={self.process_id}@{self.hostname}")
                else:
                    logger.warning(f"âš ï¸ Leaderé”é‡Šæ”¾å¤±è´¥ï¼ˆå¯èƒ½å·²è¢«æ¸…ç†ï¼‰ Leader lock release failed (may have been cleaned up): PID={self.process_id}")
                    
                self.leader_lock_acquired = False
                
        except Exception as e:
            logger.error(f"é‡Šæ”¾leaderé”å¤±è´¥ Failed to release leader lock: {e}")
    
    def _refresh_lock(self):
        """
        ğŸ”§ CRITICAL FIX: åˆ·æ–°é”çš„è¿‡æœŸæ—¶é—´ï¼ˆä½¿ç”¨SchedulerLockæ¨¡å‹ï¼‰
        Refresh lock expiration using SchedulerLock model
        """
        try:
            with app.app_context():
                # è·å–å½“å‰é”
                current_lock = SchedulerLock.get_active_lock(self.lock_key)
                if current_lock and current_lock.process_id == self.process_id:
                    # åˆ·æ–°é”
                    current_lock.refresh_lock(self.lock_timeout)
                    db.session.commit()
                    
                    logger.debug(f"ğŸ”„ é”å·²åˆ·æ–° Lock refreshed: {current_lock.to_dict()}")
                else:
                    logger.warning(f"âš ï¸ æ— æ³•åˆ·æ–°é” - é”ä¸å­˜åœ¨æˆ–ä¸å±äºå½“å‰è¿›ç¨‹ Cannot refresh lock - not owned by current process")
                    self.leader_lock_acquired = False
                
        except Exception as e:
            logger.error(f"åˆ·æ–°é”å¤±è´¥ Failed to refresh lock: {e}")
            self.leader_lock_acquired = False
    
    def _cleanup_expired_locks(self):
        """æ¸…ç†è¿‡æœŸçš„é” Cleanup expired locks"""
        try:
            current_time = datetime.utcnow()
            delete_sql = f"""
            DELETE FROM {self.lock_table_name} 
            WHERE expires_at < %(current_time)s
            """
            
            result = db.session.execute(text(delete_sql), {
                'current_time': current_time
            })
            
            if result.rowcount > 0:
                logger.info(f"ğŸ§¹ æ¸…ç†äº† {result.rowcount} ä¸ªè¿‡æœŸé” Cleaned up {result.rowcount} expired locks")
                
        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡æœŸé”å¤±è´¥ Failed to cleanup expired locks: {e}")
    
    def _get_existing_lock(self) -> Optional[Dict[str, Any]]:
        """è·å–ç°æœ‰é”ä¿¡æ¯ Get existing lock info"""
        try:
            select_sql = f"""
            SELECT lock_key, process_id, hostname, acquired_at, expires_at, last_heartbeat
            FROM {self.lock_table_name}
            WHERE lock_key = %(lock_key)s
            """
            
            result = db.session.execute(text(select_sql), {
                'lock_key': self.lock_key
            }).fetchone()
            
            if result:
                return {
                    'lock_key': result[0],
                    'process_id': result[1],
                    'hostname': result[2],
                    'acquired_at': result[3],
                    'expires_at': result[4],
                    'last_heartbeat': result[5]
                }
            return None
            
        except Exception as e:
            logger.error(f"è·å–ç°æœ‰é”ä¿¡æ¯å¤±è´¥ Failed to get existing lock info: {e}")
            return None
    
    def _is_process_alive(self, pid: int) -> bool:
        """æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜æ´» Check if process is alive"""
        try:
            return psutil.pid_exists(pid)
        except Exception:
            return False
    
    def start(self):
        """å¯åŠ¨è°ƒåº¦å™¨ Start scheduler"""
        if self.is_running:
            logger.warning("è°ƒåº¦å™¨å·²åœ¨è¿è¡Œä¸­ Scheduler already running")
            return
        
        # ğŸ”§ CRITICAL FIX: Leaderé€‰ä¸¾æœºåˆ¶é˜²æ­¢å¤šworkeré‡å¤æ‰§è¡Œ
        logger.info(f"ğŸ—³ï¸ å°è¯•è·å–leaderé” Attempting to acquire leader lock: PID={self.process_id}")
        
        if not self._acquire_leader_lock():
            logger.info(f"â³ æœªèƒ½è·å–leaderé”ï¼Œæ­¤workerå°†ä¸è¿è¡Œè°ƒåº¦å™¨ Failed to acquire leader lock, this worker will not run scheduler: PID={self.process_id}")
            return
        
        try:
            # æ³¨å†Œå®šæ—¶ä»»åŠ¡ Register scheduled tasks
            self._register_tasks()
            
            # å¯åŠ¨è°ƒåº¦å™¨çº¿ç¨‹ Start scheduler thread
            self.is_running = True
            self.scheduler_thread = threading.Thread(target=self._run_scheduler, daemon=True)
            self.scheduler_thread.start()
            
            logger.info(f"ğŸš€ åŒºå—é“¾è°ƒåº¦å™¨å·²å¯åŠ¨ Blockchain Scheduler started as LEADER: PID={self.process_id}")
            
        except Exception as e:
            logger.error(f"å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥ Failed to start scheduler: {e}")
            self.is_running = False
            self._release_leader_lock()  # é‡Šæ”¾é”
    
    def stop(self):
        """åœæ­¢è°ƒåº¦å™¨ Stop scheduler"""
        if not self.is_running:
            logger.warning("è°ƒåº¦å™¨æœªåœ¨è¿è¡Œ Scheduler not running")
            return
        
        self.is_running = False
        if self.scheduler_thread and self.scheduler_thread.is_alive():
            self.scheduler_thread.join(timeout=5)
        
        schedule.clear()
        
        # ğŸ”§ CRITICAL FIX: é‡Šæ”¾leaderé”
        self._release_leader_lock()
        
        logger.info(f"ğŸ›‘ åŒºå—é“¾è°ƒåº¦å™¨å·²åœæ­¢ Blockchain Scheduler stopped: PID={self.process_id}")
    
    def _register_tasks(self):
        """æ³¨å†Œå®šæ—¶ä»»åŠ¡ Register scheduled tasks"""
        config = self.task_config
        
        # æ¯æ—¥æ‘˜è¦è®°å½• Daily summary recording
        if config['daily_summary_recording']['enabled']:
            schedule.every().day.at(config['daily_summary_recording']['time']).do(
                self._execute_with_retry, 
                self._record_daily_summary,
                'daily_summary_recording'
            )
            logger.info(f"å·²æ³¨å†Œæ¯æ—¥æ‘˜è¦ä»»åŠ¡ Registered daily summary task at {config['daily_summary_recording']['time']}")
        
        # ç½‘ç»œçŠ¶æ€è®°å½• Network state recording
        if config['network_state_recording']['enabled']:
            schedule.every().hour.do(
                self._execute_with_retry,
                self._record_network_state,
                'network_state_recording'
            )
            logger.info("å·²æ³¨å†Œæ¯å°æ—¶ç½‘ç»œçŠ¶æ€ä»»åŠ¡ Registered hourly network state task")
        
        # æ‰¹é‡å†å²åŒæ­¥ Batch historical sync
        if config['batch_historical_sync']['enabled']:
            schedule.every().week.do(
                self._execute_with_retry,
                self._batch_historical_sync,
                'batch_historical_sync'
            )
            logger.info("å·²æ³¨å†Œæ¯å‘¨æ‰¹é‡åŒæ­¥ä»»åŠ¡ Registered weekly batch sync task")
        
        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ Data integrity check
        if config['data_integrity_check']['enabled']:
            schedule.every().day.at(config['data_integrity_check']['time']).do(
                self._execute_with_retry,
                self._check_data_integrity,
                'data_integrity_check'
            )
            logger.info(f"å·²æ³¨å†Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥ä»»åŠ¡ Registered data integrity check at {config['data_integrity_check']['time']}")
    
    def _run_scheduler(self):
        """è¿è¡Œè°ƒåº¦å™¨ä¸»å¾ªç¯ Run scheduler main loop"""
        logger.info("è°ƒåº¦å™¨ä¸»å¾ªç¯å·²å¯åŠ¨ Scheduler main loop started")
        
        last_heartbeat = datetime.utcnow()
        heartbeat_interval = 60  # æ¯60ç§’åˆ·æ–°ä¸€æ¬¡é”
        
        while self.is_running:
            try:
                # ğŸ”§ CRITICAL FIX: å®šæœŸåˆ·æ–°leaderé”ä»¥ä¿æŒæ´»è·ƒçŠ¶æ€
                current_time = datetime.utcnow()
                if (current_time - last_heartbeat).total_seconds() >= heartbeat_interval:
                    if self.leader_lock_acquired:
                        self._refresh_lock()
                        last_heartbeat = current_time
                    else:
                        logger.warning("Leaderé”å·²ä¸¢å¤±ï¼Œåœæ­¢è°ƒåº¦å™¨ Leader lock lost, stopping scheduler")
                        break
                
                schedule.run_pending()
                time.sleep(30)  # æ£€æŸ¥é—´éš”30ç§’ Check every 30 seconds
                
            except Exception as e:
                logger.error(f"è°ƒåº¦å™¨ä¸»å¾ªç¯é”™è¯¯ Scheduler main loop error: {e}")
                time.sleep(60)  # é”™è¯¯åç­‰å¾…1åˆ†é’Ÿ Wait 1 minute after error
        
        logger.info("è°ƒåº¦å™¨ä¸»å¾ªç¯å·²é€€å‡º Scheduler main loop exited")
    
    def _execute_with_retry(self, task_func, task_name: str):
        """æ‰§è¡Œä»»åŠ¡å¹¶æ”¯æŒé‡è¯• Execute task with retry support"""
        config = self.task_config.get(task_name, {})
        max_attempts = config.get('retry_attempts', 3)
        retry_delay = config.get('retry_delay', 300)
        
        for attempt in range(1, max_attempts + 1):
            try:
                logger.info(f"æ‰§è¡Œä»»åŠ¡ Executing task: {task_name} (å°è¯• attempt {attempt}/{max_attempts})")
                
                with app.app_context():
                    result = task_func()
                    
                    if result:
                        self.stats['tasks_executed'] += 1
                        self.stats['successful_records'] += 1
                        self.stats['last_execution'] = datetime.utcnow()
                        logger.info(f"ä»»åŠ¡æˆåŠŸ Task successful: {task_name}")
                        return result
                    else:
                        raise Exception(f"ä»»åŠ¡è¿”å›å¤±è´¥ Task returned failure: {task_name}")
                        
            except Exception as e:
                logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥ Task execution failed: {task_name} (å°è¯• attempt {attempt}/{max_attempts}): {e}")
                
                if attempt < max_attempts:
                    logger.info(f"ç­‰å¾…é‡è¯• Waiting for retry: {retry_delay}ç§’ seconds")
                    time.sleep(retry_delay)
                else:
                    self.stats['failed_records'] += 1
                    logger.error(f"ä»»åŠ¡æœ€ç»ˆå¤±è´¥ Task finally failed: {task_name}")
                    
                    # è®°å½•å¤±è´¥åˆ°æ•°æ®åº“ Record failure to database
                    self._record_task_failure(task_name, str(e))
    
    def _record_daily_summary(self) -> bool:
        """è®°å½•æ¯æ—¥æŒ–çŸ¿æ‘˜è¦ Record daily mining summary"""
        try:
            logger.info("å¼€å§‹è®°å½•æ¯æ—¥æŒ–çŸ¿æ‘˜è¦ Starting daily mining summary recording")
            
            # è·å–ç½‘ç»œä¿¡æ¯ Get network information
            btc_price = self._get_btc_price()
            network_info = self._get_bitcoin_network_info()
            
            if not btc_price or not network_info:
                logger.error("æ— æ³•è·å–ç½‘ç»œä¿¡æ¯ Cannot get network information")
                return False
            
            # æ„å»ºæ¯æ—¥æ‘˜è¦æ•°æ® Build daily summary data
            daily_summary = {
                "timestamp": datetime.utcnow().isoformat(),
                "date": datetime.utcnow().strftime("%Y-%m-%d"),
                "type": "daily_mining_summary",
                "btc_price": btc_price,
                "network_hashrate": network_info.get('hashrate', 0),
                "network_difficulty": network_info.get('difficulty', 0),
                "block_reward": network_info.get('block_reward', 6.25),
                "average_block_time": network_info.get('average_block_time', 600),
                "network_status": self._get_blockchain_status(),
                "recorded_by": "blockchain_scheduler_daily",
                "calculation_method": "network_summary",
                "data_source": "real_time_api"
            }
            
            # è®°å½•åˆ°åŒºå—é“¾ Record to blockchain
            result = quick_register_mining_data(daily_summary)
            
            if result:
                logger.info(f"æ¯æ—¥æ‘˜è¦å·²è®°å½• Daily summary recorded: {result['data_hash'][:16]}...")
                return True
            else:
                logger.error("æ¯æ—¥æ‘˜è¦è®°å½•å¤±è´¥ Daily summary recording failed")
                return False
                
        except Exception as e:
            logger.error(f"è®°å½•æ¯æ—¥æ‘˜è¦æ—¶å‡ºé”™ Error recording daily summary: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def _record_network_state(self) -> bool:
        """è®°å½•ç½‘ç»œçŠ¶æ€ Record network state"""
        try:
            logger.info("å¼€å§‹è®°å½•ç½‘ç»œçŠ¶æ€ Starting network state recording")
            
            # è·å–ç½‘ç»œä¿¡æ¯ Get network information
            btc_price = self._get_btc_price()
            network_info = self._get_bitcoin_network_info()
            
            if not btc_price or not network_info:
                logger.warning("æ— æ³•è·å–å®Œæ•´ç½‘ç»œä¿¡æ¯ï¼Œè·³è¿‡æ­¤æ¬¡è®°å½• Cannot get complete network info, skipping")
                return False
            
            # æ„å»ºç½‘ç»œçŠ¶æ€æ•°æ® Build network state data
            network_state = {
                "timestamp": datetime.utcnow().isoformat(),
                "hour": datetime.utcnow().strftime("%Y-%m-%d_%H"),
                "type": "hourly_network_state",
                "btc_price": btc_price,
                "network_hashrate": network_info.get('hashrate', 0),
                "network_difficulty": network_info.get('difficulty', 0),
                "block_reward": network_info.get('block_reward', 6.25),
                "recorded_by": "blockchain_scheduler_hourly",
                "calculation_method": "network_monitoring",
                "data_source": "real_time_api"
            }
            
            # è®°å½•åˆ°åŒºå—é“¾ Record to blockchain
            result = quick_register_mining_data(network_state)
            
            if result:
                logger.info(f"ç½‘ç»œçŠ¶æ€å·²è®°å½• Network state recorded: {result['data_hash'][:16]}...")
                return True
            else:
                logger.warning("ç½‘ç»œçŠ¶æ€è®°å½•å¤±è´¥ Network state recording failed")
                return False
                
        except Exception as e:
            logger.error(f"è®°å½•ç½‘ç»œçŠ¶æ€æ—¶å‡ºé”™ Error recording network state: {e}")
            return False
    
    def _batch_historical_sync(self) -> bool:
        """æ‰¹é‡å†å²æ•°æ®åŒæ­¥ Batch historical data sync"""
        try:
            logger.info("å¼€å§‹æ‰¹é‡å†å²æ•°æ®åŒæ­¥ Starting batch historical data sync")
            
            # æŸ¥æ‰¾æœªä¸Šé“¾çš„åŒºå—é“¾è®°å½• Find unrecorded blockchain records
            unrecorded_records = db.session.query(BlockchainRecord).filter(
                BlockchainRecord.verification_status == BlockchainVerificationStatus.REGISTERED,
                BlockchainRecord.transaction_hash.is_(None)
            ).limit(50).all()  # é™åˆ¶æ¯æ¬¡å¤„ç†50æ¡ Limit 50 records per batch
            
            if not unrecorded_records:
                logger.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦åŒæ­¥çš„å†å²è®°å½• No historical records found for sync")
                return True
            
            logger.info(f"æ‰¾åˆ° {len(unrecorded_records)} æ¡è®°å½•éœ€è¦åŒæ­¥ Found {len(unrecorded_records)} records for sync")
            
            # æ‰¹é‡å¤„ç† Batch processing
            sync_data = []
            for record in unrecorded_records:
                try:
                    # ä»summaryä¸­æ¢å¤æ•°æ® Restore data from summary
                    mining_data = json.loads(record.mining_data_summary) if record.mining_data_summary else {}
                    mining_data['sync_type'] = 'historical_batch'
                    mining_data['original_timestamp'] = record.data_timestamp.isoformat() if record.data_timestamp else None
                    sync_data.append(mining_data)
                    
                except Exception as record_error:
                    logger.error(f"å¤„ç†è®°å½•å¤±è´¥ Failed to process record {record.id}: {record_error}")
                    continue
            
            if sync_data:
                # æ‰¹é‡ä¸Šé“¾ Batch register to blockchain
                batch_result = self._batch_register_mining_data(sync_data)
                
                if batch_result:
                    logger.info(f"æ‰¹é‡åŒæ­¥æˆåŠŸ Batch sync successful: {len(batch_result)} æ¡è®°å½• records")
                    return True
                else:
                    logger.error("æ‰¹é‡åŒæ­¥å¤±è´¥ Batch sync failed")
                    return False
            else:
                logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯åŒæ­¥ No valid data for sync")
                return False
                
        except Exception as e:
            logger.error(f"æ‰¹é‡å†å²æ•°æ®åŒæ­¥æ—¶å‡ºé”™ Error in batch historical sync: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def _check_data_integrity(self) -> bool:
        """æ£€æŸ¥æ•°æ®å®Œæ•´æ€§ Check data integrity"""
        try:
            logger.info("å¼€å§‹æ•°æ®å®Œæ•´æ€§æ£€æŸ¥ Starting data integrity check")
            
            # æŸ¥è¯¢æœ€è¿‘7å¤©çš„è®°å½• Query records from last 7 days
            seven_days_ago = datetime.utcnow() - timedelta(days=7)
            recent_records = db.session.query(BlockchainRecord).filter(
                BlockchainRecord.data_timestamp >= seven_days_ago,
                BlockchainRecord.verification_status == BlockchainVerificationStatus.REGISTERED
            ).all()
            
            verified_count = 0
            failed_count = 0
            
            for record in recent_records:
                try:
                    # éªŒè¯åŒºå—é“¾æ•°æ® Verify blockchain data
                    if record.data_hash and record.ipfs_cid:
                        verification_result = self._verify_blockchain_data(
                            record.data_hash, 
                            record.ipfs_cid
                        )
                        
                        if verification_result and verification_result.get('is_valid', False):
                            verified_count += 1
                            
                            # æ›´æ–°éªŒè¯çŠ¶æ€ Update verification status
                            record.verification_status = BlockchainVerificationStatus.VERIFIED
                        else:
                            failed_count += 1
                            logger.warning(f"æ•°æ®éªŒè¯å¤±è´¥ Data verification failed for record {record.id}")
                            record.verification_status = BlockchainVerificationStatus.FAILED
                    
                except Exception as verify_error:
                    logger.error(f"éªŒè¯è®°å½•å¤±è´¥ Failed to verify record {record.id}: {verify_error}")
                    failed_count += 1
                    continue
            
            # æäº¤æ•°æ®åº“æ›´æ”¹ Commit database changes
            db.session.commit()
            
            logger.info(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ Data integrity check completed: {verified_count} éªŒè¯æˆåŠŸ verified, {failed_count} å¤±è´¥ failed")
            
            # è®°å½•æ£€æŸ¥ç»“æœ Record check results
            integrity_summary = {
                "timestamp": datetime.utcnow().isoformat(),
                "type": "data_integrity_check",
                "total_checked": len(recent_records),
                "verified_count": verified_count,
                "failed_count": failed_count,
                "check_period_days": 7,
                "recorded_by": "blockchain_scheduler_integrity",
                "calculation_method": "data_verification",
                "data_source": "blockchain_verification"
            }
            
            # å°†æ£€æŸ¥ç»“æœä¹Ÿä¸Šé“¾ Record check results to blockchain
            result = quick_register_mining_data(integrity_summary)
            
            if result:
                logger.info(f"å®Œæ•´æ€§æ£€æŸ¥ç»“æœå·²è®°å½• Integrity check results recorded: {result['data_hash'][:16]}...")
            
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®å®Œæ•´æ€§æ£€æŸ¥æ—¶å‡ºé”™ Error in data integrity check: {e}")
            logger.error(traceback.format_exc())
            return False
    
    def _record_task_failure(self, task_name: str, error_message: str):
        """è®°å½•ä»»åŠ¡å¤±è´¥ä¿¡æ¯ Record task failure information"""
        try:
            with app.app_context():
                failure_record = {
                    "timestamp": datetime.utcnow().isoformat(),
                    "type": "scheduler_task_failure",
                    "task_name": task_name,
                    "error_message": error_message,
                    "recorded_by": "blockchain_scheduler_error",
                    "calculation_method": "error_logging",
                    "data_source": "scheduler_internal"
                }
                
                # å°è¯•è®°å½•é”™è¯¯åˆ°åŒºå—é“¾ Try to record error to blockchain
                result = quick_register_mining_data(failure_record)
                
                if result:
                    logger.info(f"ä»»åŠ¡å¤±è´¥å·²è®°å½•åˆ°åŒºå—é“¾ Task failure recorded to blockchain: {result['data_hash'][:16]}...")
                else:
                    logger.warning("æ— æ³•å°†ä»»åŠ¡å¤±è´¥è®°å½•åˆ°åŒºå—é“¾ Cannot record task failure to blockchain")
                    
        except Exception as e:
            logger.error(f"è®°å½•ä»»åŠ¡å¤±è´¥ä¿¡æ¯æ—¶å‡ºé”™ Error recording task failure: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–è°ƒåº¦å™¨ç»Ÿè®¡ä¿¡æ¯ Get scheduler statistics"""
        uptime = datetime.utcnow() - self.stats['uptime_start']
        
        return {
            'is_running': self.is_running,
            'uptime_seconds': uptime.total_seconds(),
            'uptime_hours': uptime.total_seconds() / 3600,
            'tasks_executed': self.stats['tasks_executed'],
            'successful_records': self.stats['successful_records'],
            'failed_records': self.stats['failed_records'],
            'success_rate': (self.stats['successful_records'] / max(self.stats['tasks_executed'], 1)) * 100,
            'last_execution': self.stats['last_execution'].isoformat() if self.stats['last_execution'] else None,
            'task_config': self.task_config
        }
    
    def force_execute_task(self, task_name: str) -> bool:
        """å¼ºåˆ¶æ‰§è¡ŒæŒ‡å®šä»»åŠ¡ Force execute specific task"""
        task_methods = {
            'daily_summary_recording': self._record_daily_summary,
            'network_state_recording': self._record_network_state,
            'batch_historical_sync': self._batch_historical_sync,
            'data_integrity_check': self._check_data_integrity
        }
        
        if task_name not in task_methods:
            logger.error(f"æœªçŸ¥ä»»åŠ¡ Unknown task: {task_name}")
            return False
        
        logger.info(f"å¼ºåˆ¶æ‰§è¡Œä»»åŠ¡ Force executing task: {task_name}")
        
        try:
            with app.app_context():
                result = task_methods[task_name]()
                if result:
                    logger.info(f"å¼ºåˆ¶æ‰§è¡ŒæˆåŠŸ Force execution successful: {task_name}")
                else:
                    logger.error(f"å¼ºåˆ¶æ‰§è¡Œå¤±è´¥ Force execution failed: {task_name}")
                return result
                
        except Exception as e:
            logger.error(f"å¼ºåˆ¶æ‰§è¡Œä»»åŠ¡æ—¶å‡ºé”™ Error in force execution: {task_name}: {e}")
            return False
    
    # è¾…åŠ©æ–¹æ³• Helper methods
    def _get_btc_price(self) -> Optional[float]:
        """è·å–BTCä»·æ ¼ Get BTC price"""
        try:
            if API_CLIENT_AVAILABLE:
                return api_client.get_btc_price()
            else:
                logger.warning("APIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ä»·æ ¼ API client unavailable, using default price")
                return 80000.0  # é»˜è®¤ä»·æ ¼
        except Exception as e:
            logger.error(f"è·å–BTCä»·æ ¼å¤±è´¥ Failed to get BTC price: {e}")
            return 80000.0
    
    def _get_bitcoin_network_info(self) -> Optional[Dict[str, Any]]:
        """è·å–æ¯”ç‰¹å¸ç½‘ç»œä¿¡æ¯ Get Bitcoin network info"""
        try:
            if API_CLIENT_AVAILABLE:
                market_data = api_client.get_market_data()
                return {
                    'hashrate': market_data.get('network_hashrate', 900.0),  # EH/s
                    'difficulty': market_data.get('network_difficulty', 119.12),
                    'block_reward': 3.125,  # Current block reward after 2024 halving
                    'average_block_time': 600  # 10 minutes in seconds
                }
            else:
                logger.warning("APIå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ç½‘ç»œä¿¡æ¯ API client unavailable, using default network info")
                return {
                    'hashrate': 900.0,
                    'difficulty': 119.12,
                    'block_reward': 3.125,
                    'average_block_time': 600
                }
        except Exception as e:
            logger.error(f"è·å–ç½‘ç»œä¿¡æ¯å¤±è´¥ Failed to get network info: {e}")
            return None
    
    def _get_blockchain_status(self) -> str:
        """è·å–åŒºå—é“¾çŠ¶æ€ Get blockchain status"""
        try:
            if BLOCKCHAIN_AVAILABLE:
                integration = get_blockchain_integration()
                if integration and integration.w3 and integration.w3.is_connected():
                    return "connected"
                else:
                    return "disconnected"
            else:
                return "unavailable"
        except Exception as e:
            logger.error(f"è·å–åŒºå—é“¾çŠ¶æ€å¤±è´¥ Failed to get blockchain status: {e}")
            return "error"
    
    def _batch_register_mining_data(self, batch_data: List[Dict[str, Any]]) -> Optional[List[Dict[str, Any]]]:
        """æ‰¹é‡æ³¨å†ŒæŒ–çŸ¿æ•°æ® Batch register mining data"""
        try:
            results = []
            for data in batch_data:
                result = quick_register_mining_data(data)
                if result:
                    results.append(result)
                else:
                    logger.warning(f"å•ä¸ªæ•°æ®è®°å½•å¤±è´¥ Individual data record failed: {data.get('timestamp', 'unknown')}")
            
            return results if results else None
            
        except Exception as e:
            logger.error(f"æ‰¹é‡æ³¨å†Œå¤±è´¥ Batch registration failed: {e}")
            return None
    
    def _verify_blockchain_data(self, data_hash: str, ipfs_cid: str) -> Optional[Dict[str, Any]]:
        """éªŒè¯åŒºå—é“¾æ•°æ® Verify blockchain data"""
        try:
            # ä½¿ç”¨quick_verify_mining_dataè¿›è¡ŒéªŒè¯
            result = quick_verify_mining_data(data_hash)
            if result and result.get('valid', False):
                return {
                    'is_valid': True,
                    'data_hash': data_hash,
                    'ipfs_cid': ipfs_cid,
                    'verification_result': result
                }
            else:
                return {
                    'is_valid': False,
                    'data_hash': data_hash,
                    'ipfs_cid': ipfs_cid,
                    'error': result.get('error', 'Verification failed')
                }
                
        except Exception as e:
            logger.error(f"æ•°æ®éªŒè¯å¤±è´¥ Data verification failed: {e}")
            return {
                'is_valid': False,
                'data_hash': data_hash,
                'ipfs_cid': ipfs_cid,
                'error': str(e)
            }


# å…¨å±€è°ƒåº¦å™¨å®ä¾‹ Global scheduler instance
scheduler_instance = None

def get_scheduler() -> BlockchainScheduler:
    """è·å–è°ƒåº¦å™¨å®ä¾‹ Get scheduler instance"""
    global scheduler_instance
    if scheduler_instance is None:
        scheduler_instance = BlockchainScheduler()
    return scheduler_instance

def start_blockchain_scheduler():
    """å¯åŠ¨åŒºå—é“¾è°ƒåº¦å™¨ Start blockchain scheduler"""
    scheduler = get_scheduler()
    scheduler.start()
    return scheduler

def stop_blockchain_scheduler():
    """åœæ­¢åŒºå—é“¾è°ƒåº¦å™¨ Stop blockchain scheduler"""
    scheduler = get_scheduler()
    scheduler.stop()

if __name__ == "__main__":
    # æµ‹è¯•è¿è¡Œ Test run
    logging.basicConfig(level=logging.DEBUG)
    
    print("å¯åŠ¨åŒºå—é“¾è°ƒåº¦å™¨æµ‹è¯• Starting blockchain scheduler test...")
    
    scheduler = BlockchainScheduler()
    
    try:
        scheduler.start()
        print("è°ƒåº¦å™¨å·²å¯åŠ¨ï¼ŒæŒ‰Ctrl+Cåœæ­¢ Scheduler started, press Ctrl+C to stop")
        
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\næ­£åœ¨åœæ­¢è°ƒåº¦å™¨ Stopping scheduler...")
        scheduler.stop()
        print("è°ƒåº¦å™¨å·²åœæ­¢ Scheduler stopped")